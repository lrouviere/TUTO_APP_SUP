[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Apprentissage supervisé avec R",
    "section": "",
    "text": "Présentation\nCe tutoriel présente les principaux algorithmes d’apprentissage supervisé avec R. On pourra trouver :\n\nles supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/page_perso/apprentissage_sup.html ;\nle tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_APP_SUP\nle tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_APP_SUP/correction.\n\nLes thèmes suivants sont abordés :\n\nEstimation du risque, présentation du metapackage tidymodels ;\nSVM, cas séparable, non séparable et astuce du noyau ;\nArbres, notamment l’algorithme CART ;\nAgrégation d’arbres, forêts aléatoires et gradient boosting ;\nRéseaux de neurones et introduction au deep learning, perceptron multicouches avec keras.\n\nIl existe de nombreuses références sur le machine learning, la plus connue étant certainement Hastie, Tibshirani, et Friedman (2009), disponible en ligne à l’url https://web.stanford.edu/~hastie/ElemStatLearn/. On pourra également consulter Boehmke et Greenwell (2019) qui propose une présentation très claire des algorithmes machine learning avec R. Cet ouvrage est également disponible en ligne à l’url https://bradleyboehmke.github.io/HOML/.\n\n\n\n\nBoehmke, B., et B. Greenwell. 2019. Hands-On Machine Learning with R. CRC Press. https://bradleyboehmke.github.io/HOML/.\n\n\nHastie, T., R. Tibshirani, et J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/."
  },
  {
    "objectID": "01-risque.html#lalgorithme-des-k-plus-proches-voisins",
    "href": "01-risque.html#lalgorithme-des-k-plus-proches-voisins",
    "title": "1  Calcul de risques avec R",
    "section": "1.1 L’algorithme des \\(k\\) plus proches voisins",
    "text": "1.1 L’algorithme des \\(k\\) plus proches voisins\nC’est un algorithme simple qui va établir des prévisions en point \\(x\\) à partir des observations qui sont proches de \\(x\\). Plus précisément, étant donné \\(k\\leq n\\) il est défini par \\[g_{n,k}(x)=\\left\\{\n\\begin{array}{ll}\n1 & \\text{si }\\sum_{i\\in\\text{kppv}(x)}\\mathbf 1_{y_i=1}\\geq \\sum_{i\\in\\text{kppv}(x)}\\mathbf 1_{y_i=0} \\\\\n0 & \\text{sinon.}\n\\end{array}\\right.\\] pour la prévision des groupes et par \\[S_{n,k}(x)=\\frac{1}{|{\\text{kppv}(x)}|}\\sum_{i\\in\\text{kppv}(x)}\\mathbf 1_{y_i=1}\\] pour la prévision de la probabilité \\(\\mathbf P(Y=1|X=x)\\) avec \\[\\text{kppv}(x)=\\{i\\leq n:\\|x-x_i\\|\\leq \\|x-x_{(k)}\\|\\}\\] et \\(\\|x-x_{(k)}\\|\\) la \\(k^e\\) plus petite valeur parmi \\(\\{\\|x-x_1\\|,\\dots,\\|x-x_n\\|\\}\\).\n\nExercice 1.1 (kppv avec kknn) \n\n\nSéparer les données en un échantillon d’apprentissage de taille 300 et un échantillon test de taille 200.\nA l’aide de la fonction kknn du package kknn entraîner la règle des 3 plus proches voisins sur les données d’apprentissage et calculer les groupes prédits par cette règle des données test.\nEn déduire une estimation de l’erreur de classification de la règle des 3 plus proches voisins.\n\nIl suffit de calculer la proportion de mauvaises prévisions sur les données test :\n\nRetrouver le résultat précédent à l’aide la fonction accuracy du package yardstick.\nToujours à l’aide des fonctiosn de yardstick, calculer la spécificité, le sensibilité et le balanced accuracy. On pourra consulter la page https://yardstick.tidymodels.org/articles/metric-types.html#metrics\nRetrouver les résultats précédents en utilisant la fonction metric_set."
  },
  {
    "objectID": "01-risque.html#la-courbe-roc",
    "href": "01-risque.html#la-courbe-roc",
    "title": "1  Calcul de risques avec R",
    "section": "1.2 La courbe ROC",
    "text": "1.2 La courbe ROC\nC’est un critère fréquemment utilisé pour mesurer la performance d’un score. Etant donné \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathcal X\\times \\{-1,1\\}\\), on rappelle qu’un score est une fonction \\(S:\\mathcal X\\to\\mathbb R\\). Dans la plupart des cas, un score s’obtient en estimant la probabilité \\(\\mathbf P(Y=1|X=x)\\). Pour un seuil \\(s\\in\\mathbb R\\) fixé, un score possède deux types d’erreur \\[\\alpha(s)=\\mathbf P(S(X)\\geq s|Y=-1)\\quad\\text{et}\\quad\\beta(s)=\\mathbf P(S(X)&lt; s|Y=1).\\] La courbe ROC est la courbe paramétrée définie par : \\[\\left\\{\n\\begin{array}{l}\nx(s)=\\alpha(s)=\\mathbf P(S(X)&gt;s|Y=-1) \\\\\ny(s)=1-\\beta(s)=\\mathbf P(S(X)\\geq s|Y=1)\n\\end{array}\\right.\\] Elle permet donc de visualiser sur une seul graphe 2D ces deux erreurs pour toutes les valeurs de seuil \\(s\\).\n\nExercice 1.2 (Étude de la courbe ROC) On considère dans cet exercice une fonction de score \\(S\\) que l’on suppose absolument continue.\n\nMontrer que la courbe ROC vit dans le carré \\([0,1]^2\\).\n\nOn note \\(x(-\\infty)=\\lim_{n\\to -\\infty}x(s)\\). On a \\[x(-\\infty)=\\lim_{n\\to -\\infty}\\mathbf P(S(X)&gt; s|Y=-1)=1.\\] De même on montre facilement \\(y(-\\infty)=1\\), \\(x(\\infty)=0\\) et \\(y(\\infty)=0\\). On déduit donc que la courbe ROC part du point \\((1,1)\\) pour arriver au point \\((0,0)\\) sans quitter le carré \\([0,1]^2\\).\n\nOn suppose dans cette question que \\(S\\) est parfait, ce qui revient à dire qu’il sépare parfaitement les 2 groupes. Mathématiquement on traduit cela par l’existence d’un seuil \\(s^\\star\\in\\mathbb R\\) tel que \\[\\mathbf P(Y=1|S(X)\\geq s^\\star)=1\\quad\\textrm{et}\\quad\\mathbf P(Y=-1|S(X)&lt;s^\\star)=1.\\] Analyser la courbe ROC du score parfait.\n\nIl est facile de voir que \\(\\alpha(s^\\star)=\\beta(s^\\star)=0\\). Par conséquent la courbe ROC du score parfait passe par le point \\((0,1)\\). On déduit que la courbe ROC du score parfait est l’union des segments \\[[(0,0),(1,0)]\\quad\\text{et}\\quad[(1,0),(1,1)].\\]\n\nOn suppose dans cette question que \\(S\\) est aléatoire dans le sens où \\(S(X)\\) est indépendante de \\(Y\\) (cela revient à dire que les notes \\(S(X)\\) n’ont aucun lien avec le groupe). Analyser la courbe ROC d’un tel score.\n\nEn utilisant l’indépendance et l’absolu continuité de \\(S(X)\\) on a \\[\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s|Y=-1)=\\mathbf P(S(X)&gt; s)=\\mathbf P(S(X)\\geq s) \\\\\n=& \\mathbf P(S(X)\\geq s|Y=1)=y(s),\n\\end{aligned}\n\\] On déduit que la courbe ROC est la première bissectrice.\n\n\n\n\nExercice 1.3 (Courbe ROC avec yardstick) On reprend les données de l’Exercice 1.1 et on considère toujours la règle des 3 plus proches voisins entrainée sur les données d’apprentissage.\n\nToujours pour la règle des 3 plus proches voisins, calculer les probabilités \\(\\mathbf P(Y=1|X=x)\\) prédites pour les données tests.\nEn déduire une estimation de la courbe ROC de la règle des 3 plus proches voisins. On pourra utiliser la fonction roc_curve du package yardstick.\n\n\n\nExercice 1.4 (Autres outils R pour la courbe ROC) On dispose de 4 fonctions de score \\(S_j(x)\\) dont on souhaite visualiser les courbes ROC à partir des valeurs de score calculés sur un échantillon. On trouvera dans le tableau df les scores \\(S_j(X_i),i=1,\\dots,n\\) ainsi que les observations des groupes \\(Y_i\\)\n\nset.seed(12345)\nn &lt;- 200\nY &lt;- rbinom(n,1,0.5)\nS1 &lt;- runif(n)\nS2 &lt;- S1\nS2[Y==1] &lt;- runif(sum(Y==1),0.6,1)\nS2[Y==0] &lt;- runif(sum(Y==0),0,0.6)\nS3 &lt;- S2\nS3[Y==1][1:10] &lt;- runif(10,0,0.6)\nS3[Y==0][1:10] &lt;- runif(10,0.6,1)\n(tbl &lt;- tibble(S1,S2,S3,Y=Y))\n## # A tibble: 200 × 4\n##        S1    S2      S3     Y\n##     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n##  1 0.589  0.630 0.420       1\n##  2 0.893  0.790 0.00624     1\n##  3 0.124  0.706 0.302       1\n##  4 0.513  0.692 0.439       1\n##  5 0.664  0.203 0.806       0\n##  6 0.766  0.404 0.666       0\n##  7 0.0926 0.194 0.954       0\n##  8 0.0676 0.838 0.198       1\n##  9 0.556  0.664 0.492       1\n## 10 0.651  0.942 0.531       1\n## # ℹ 190 more rows\n\n\nVisualiser, pour chaque score, les valeurs de score en fonction de \\(Y\\). Commenter\n\nOn remarque que S2 sépare parfaitement les deux groupes. La séparation de S3 est satisfaisante mais pas parfaite. S1 semble quant à lui ordonner les observations indépendamment du groupe (proche du score aléatoire).\n\nVisualiser sur un même graphe les trois courbes ROC. On pourra utiliser d’abord utiliser la fonction roc du package pROC puis la fonction geom_roc du plotROC et enfin la fonction roc_curve du package yardstick.\n\nAvec pROC on peut combiner les fonction roc et plot.\n\n\npROC possède également une fonction ggroc qui permet d’obtenir les courbes ROC en ggplot.\n\n\nPour plotROC on utilise le verbe geom_roc :\n\n\nEnfin avec roc_curve :\n\nCalculer les AUC à l’aide de la fonction aucde pROC puis roc_auc de yardstick.\n\nOn peut aussi les obtenir avec la syntaxe dplyr\n\n\nOn peut obtenir les mêmes valeurs avec roc_auc :\n\nOn rappelle que l’AUC vérifier la propriété suivante : si \\((X_1,Y_1)\\) et \\((X_2,Y_2)\\) sont indépendantes et de même loi que \\((X,Y)\\) et que \\(S(X)\\) est continue, on a \\[AUC(S)=\\mathbf P(S(X_1)\\geq S(X_2)|(Y_1,Y_2)=(1,-1)).\\] Utiliser cette propriété pour retrouver l’AUC de S3.\n\nOn peut estimer l’AUC en calculant, parmi les paires qui vérifient \\((Y_1,Y_2)=(1,-1)\\), la proportion de paires qui vérifient \\(S(X_1)\\lt S(X_2)\\). Si on note \\[\\mathcal I=\\{(i,j),y_i=1,y_j=0\\},\\] alors l’estimateur s’écrit \\[\\widehat{AUC}(S)=\\frac{1}{|\\mathcal I|}\\sum_{(i,j)\\in\\mathcal I}\\mathbf 1_{S(X_i)&gt;S(S_j)}.\\]"
  },
  {
    "objectID": "01-risque.html#exercices-complémentaires",
    "href": "01-risque.html#exercices-complémentaires",
    "title": "1  Calcul de risques avec R",
    "section": "1.3 Exercices complémentaires",
    "text": "1.3 Exercices complémentaires\n\nExercice 1.5 (Optimalité de la fonction de régression) Soit \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathbb R^d\\times\\mathbb R\\).\n\nRappeler la définition du risque quadratique en régression.\n\nIl est défini par \\[\\mathcal R(m)=\\mathbf E\\big[(Y-m(X))^2\\big].\\]\n\nMontrer que la fonction de régression \\(m^\\star(x)=\\mathbf E[Y|X=x]\\) minimise ce risque.\n\nPour toute fonction \\(m:\\mathbb R^d\\to\\mathbb R\\), on a\n\\[\n\\begin{aligned}\n   \\mathcal R(m)= & \\mathbf E[(Y-m(X))^2] \\nonumber \\\\\n   = & \\mathbf E[(Y-m^\\star(X)+m^\\star(X)-m(X))^2] \\nonumber \\\\\n   = & \\mathbf E[(Y-m^\\star(X))^2]+\\mathbf E[(m^\\star(X)-m(X))^2]\n\\end{aligned}\n\\tag{1.1}\\]\ncar\n\\[\n\\begin{aligned}\n   \\mathbf E[(m^\\star(X)-m(X)) & (Y-m^\\star(X))] \\\\\n   = & \\mathbf E\\left[\\mathbf E[(m^\\star(X)-m(X))(Y-m^\\star(X))|X]\\right] \\\\\n   = & \\mathbf E\\left[\\mathbf E[(m^\\star(X)-m(X))]\\mathbf E[Y-m^\\star(X)|X]\\right] \\\\\n   = & \\mathbf E\\left[\\mathbf E[(m^\\star(X)-m(X))](m^\\star(X)-m^\\star(X))\\right] \\\\\n   = & 0.\n\\end{aligned}\n\\] On conclut de l’équation 1.1 que le risque est minimal en \\(m^\\star\\).\n\n\n\n\nExercice 1.6 (Optimalité de la règle de Bayes) Soit \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathbb R^d\\times\\{0,1\\}\\).\n\nRappeler la définition d’une fonction de prévision.\n\nC’est une fonction \\(g:\\mathbb R^p\\to\\{0,1\\}\\).\n\nRappeler la définition de la règle de Bayes \\(g^\\star\\) et de l’erreur de Bayes \\(L^\\star\\).\n\nLa règle de Bayes est définie par \\[g^\\star(x)=\\left\\{\n\\begin{array}{ll}\n1& \\text{si }\\mathbf P(Y=1|X=x)\\geq 0.5 \\\\\n0& \\text{sinon.}\n\\end{array}\\right.\\] L’erreur de Bayes est définie par \\(L^\\star=\\mathbf P(g^\\star(X)\\neq Y)\\).\n\nSoit \\(g\\) une fonction de décision. Montrer que pour tout \\(x\\in\\mathbb R^d\\) on a \\[\\mathbf P(g(X)\\neq Y|X=x)=1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x)))\\] où \\(\\eta(x)=\\mathbf P(Y=1|X=x)\\).\n\nOn a\n\\[\n\\begin{aligned}\n  \\mathbf P(g(X)\\neq Y|X=x) & = 1-\\Big(\\mathbf P(g(X)=Y,g(X)=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf P(g(X)=Y,g(X)=0|X=x)\\Big) \\\\\n  & = 1-\\Big(\\mathbf 1_{g(x)=1}\\mathbf P(Y=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf 1_{g(x)=0}\\mathbf P(Y=0|X=x)\\Big) \\\\\n  & = 1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x))).\n\\end{aligned}\n\\]\n\nEn déduire que pour tout \\(x\\in\\mathbb R^d\\) et pour toute fonction de prévision \\(g\\) \\[\\mathbf P(g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x)\\geq 0.\\] Conclure.\n\nOn déduit\n\\[\n\\begin{aligned}\n  \\mathbf P(&  g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x) \\\\\n  &  = \\eta(x)\\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right)+(1-\\eta(x))\\left(\\mathbf 1_{g^\\star(x)=0}-\\mathbf 1_{g(x)=0}\\right) \\\\\n  &  = (2\\eta(x)-1) \\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right) \\\\\n  &  \\geq 0\n\\end{aligned}\n\\]\npar définition de \\(g^\\star\\). On conclut en intégrant par la loi de \\(X\\) que \\[\\mathbf P(g(X)\\neq Y)\\geq \\mathbf P(g^\\star(X)\\neq Y).\\]\n\nOn considère \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathbb R\\times\\{0,1\\}\\) tel que\n\\[\nX\\sim\\mathcal U[-2,2]\\quad\\text{et}\\quad (Y|X=x)\\sim\\left\\{\n\\begin{array}{ll}\n\\mathcal B(1/5) & \\textrm{si } x\\leq 0 \\\\\n\\mathcal B(9/10) & \\textrm{si } x&gt;0\n\\end{array}\\right.\n\\]\noù \\(\\mathcal U[a,b]\\) désigne la loi uniforme sur \\([a,b]\\) et \\(\\mathcal B(p)\\) la loi de Bernoulli de paramètre \\(p\\). Calculer la règle de Bayes et l’erreur de Bayes.\n\nSi \\(x\\leq 0\\), on a \\(\\mathbf P(Y=1|X=x)=\\frac{1}{5}\\) et si \\(x&gt; 0\\), \\(\\mathbf P(Y=1|X=x)=\\frac{9}{10}\\). Ainsi \\[g^\\star(x)=\\left\\{\n\\begin{array}{ll}\n0 &  \\text{si }x\\leq 0\\\\\n1 &  \\text{si }x&gt; 0.\n\\end{array}\\right.\n\\] L’erreur de Bayes vaut\n\\[\n\\begin{aligned}\n  L^\\star=\\mathbf P(g^\\star(X)\\neq Y&  |X\\leq 0)\\mathbf P(X\\leq 0) \\\\\n  &  +\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)\\mathbf P(X&gt;0).\n\\end{aligned}\n\\]\nOr \\[\\mathbf P(g^\\star(X)\\neq Y|X\\leq 0)=\\mathbf P(Y\\neq 0|X\\leq 0)=\\frac{1}{5}\\] et \\[\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)=\\mathbf P(Y\\neq 1|X&gt; 0)=\\frac{1}{10}.\\] On obtient \\[L^\\star=\\frac{1}{5}\\,\\frac{1}{2}+\\frac{1}{10}\\,\\frac{1}{2}=\\frac{3}{20}.\\]"
  },
  {
    "objectID": "02-est-risque.html#validation-croisée",
    "href": "02-est-risque.html#validation-croisée",
    "title": "2  Estimation du risque par ré-échantillonnage",
    "section": "2.1 Validation croisée",
    "text": "2.1 Validation croisée\n\nExercice 2.1 (Calculs de blocs avec rsample) Le package rsample permet d’obtenir des partitions des données pour calculer des estimations de risques. On s’intéresse dans cet exercice aux fonctions\n\nmc_cv pour partager les données en 2 (validation hold out)\nvfold_cv pour partager les données en blocs (validation croisée)\n\nOn reprend les données générées avec la fonction du chapitre précédent :\n\ndonnees &lt;- gen_class_bin2D(n=500,graine=12345,bayes=0.20)$donnees\n\n\nÀ l’aide de mc_cv séparer les données en 2 échantillons apprentissage/test avec 2/3 des données dans l’échantillon d’apprentissage.\nAfficher l’index des individus de l’échantillon d’apprentissage. Faire de même pour l’échantillon test.\n\nPour l’échantillon test, il suffit de considérer les individus qui ne sont pas dans l’échantillon d’apprentissage\n\nÀ l’aide de vfold_cv séparer les données en 10 blocs.\nAfficher l’index des individus qui se trouvent dans le bloc 4.\n\n\nOn considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. On s’intéresse à l’estimation de ce risque par des méthodes de ré-échantillonnage pour l’algorithme des \\(k\\) plus proches voisins.\n\nExercice 2.2 (Progammation de validation hold out et validation croisée) \n\n\nCréer une fonction qui admet en entrée\n\nun jeu de données\nune valeur de k\nune séparation des données en apprentissage/test issue de mc_cv\n\net renvoie en sortie l’erreur de classification de la règle des \\(k\\) plus proches voisins calculées sur l’échantillon test. On pourra la tester pour k=3.\nRefaire la question précédente avec une validation croisée K blocs.\nOn considère la grille de valeurs de k\n\ngrille.k &lt;- c(1:100)\n\nSélectionner dans cette grille une valeur de \\(k\\) en minimisant l’erreur de classification calculée par validation hold out, puis par validation croisée 10 blocs. On pourra également visualiser les courbes de risque (erreur de classification en fonction du nombre de voisins).\n\nIl suffit de calculer les erreurs pour chaque valeur de k avec une boucle\n\n\nOn peut les visualiser\n\n\net en déduire les valeurs optimales"
  },
  {
    "objectID": "02-est-risque.html#le-package-tidymodels",
    "href": "02-est-risque.html#le-package-tidymodels",
    "title": "2  Estimation du risque par ré-échantillonnage",
    "section": "2.2 Le package tidymodels",
    "text": "2.2 Le package tidymodels\nIl s’agit d’un meta-package qui regroupe plusieurs packages (parsnip, recipes, rsample…) qui utilisent une approche tidy pour sélectionner un algorithme de prévision. Il propose de créer un projet de travail, appelé workflow, permettant de construire un algorithme de prévision. Cette construction requiert tout un ensemble d’étapes comme la gestion des données manquantes, la création ou suppression de variables explicatives, la recherche de colinéarité parmi les variables explicatives, la sélection des paramètres de l’algorithme… Toutes ces étapes s’enchaînent en mettant à jour le workflow. Nous ne présentons pas toutes les fonctionnalités de tidymodels, on pourra les retrouver dans le tutoriel présentant le package à l’url https://www.tidymodels.org. Nous nous concentrons uniquement sur le choix de paramètres d’algorithmes de prévision à travers l’exemple des plus proches voisins.\nLa première étape est consacrée à la définition de l’algorithme. Pour des plus proches voisins en classification on utilise\n\nlibrary(tidymodels)\ntune_spec &lt;- \n  nearest_neighbor(neighbors=tune(),weight_func=\"rectangular\") |&gt; \n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\") \n\nDans la fonction nearest_neighbor nous précisions que nous souhaitons utiliser le noyau rectangular comme nous l’avons fait jusqu’à présent. L’option neighbors=tune() spécifie que le nombre de plus proches voisins est un paramètre à calibrer. Plusieurs packages R permettent d’entraîner des \\(k\\) plus proches voisins, on utilise set_engine(\"kknn\") pour indiquer que l’on souhaite utiliser la fonction kknn. On pourra trouver l’ensemble des algorithmes disponibles dans tidymodels ainsi que les noms de leurs paramètres ici : https://www.tidymodels.org/find/parsnip/.\nOn initialise le projet de travail dans la seconde étape en créant le workflow :\n\nppv_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(Y ~ .)\n\nOn renseigne dans add_model les différentes informations sur l’algorithme des plus proches voisins et on indique dans add_formula la variable à expliquer et les variables explicatives. Nous avons vu dans la section précédente que la recherche du meilleur nombre de voisins s’effectue en estimant un risque de prévision sur une grille de valeurs candidates. On considère toujours des valeurs de \\(k\\) variant entre 1 et 100 :\n\ngrille_k &lt;- tibble(neighbors=1:100)\n\nLe choix de \\(k\\) s’effectue avec la fonction tune_grid du package tune qui possède notamment les arguments suivants :\n\ntune_grid(...,resamples=...,grid=...,metrics=...)\n\nOn indique dans :\n\nresamples le ré-échantillonnage. Par exemple les échantillons d’apprentissage et test pour la validation hold out (avec la fonction mc_cv), les blocs pour la validation croisée (vfolds_cv), les tirages bootstraps (bootstraps) ;\ngrid la grille de paramètres candidats ;\nmetrics le risque de prévision (erreur quadratique, erreur de classification, \\(\\text{AUC}\\)…).\n\n\nExercice 2.3 (Validation hold out et validation croisée avec tidymodels) \n\n\nRetrouver les erreurs par validation hold out en utilisant tune_grid et collect_metrics.\nVisualiser les meilleures valeurs de k à l’aide de show_best\nSélectionner la meilleur valeur de k avec select_best.\nFinaliser l’algorithme à l’aide de finalize_workflow et fit.\nFaire le même travail pour la validation croisée 10 blocs.\n\nOn obtient la valeur de k sélectionnée avec\n\n\n\n\nExercice 2.4 (Validation croisée pour l’AUC) Sélectionner la valeur de \\(k\\) qui maximise l’AUC calculé par validation croisée 10 blocs.\n\nOn a ajouté control=control_resamples(save_pred = TRUE) pour stocker les valeurs prédites et tracer les courbes ROC si besoin. La meilleur valeur de \\(k\\) s’obtient toujours avec :\n\n\nLa courbe ROC pour la valeur sélectionnée s’obtient avec"
  },
  {
    "objectID": "03-comp-risque.html#variance-de-lerreur-de-validation-croisée",
    "href": "03-comp-risque.html#variance-de-lerreur-de-validation-croisée",
    "title": "3  Compléments",
    "section": "3.1 Variance de l’erreur de validation croisée",
    "text": "3.1 Variance de l’erreur de validation croisée\nLorsque les estimateurs du risque sont obtenus en ajustant l’algorithme plusieurs fois (sur différent sous-échantillons), il est possible d’estimer la variance (conditionnelle aux données) de ces estimateurs. Par exemple, dans le cas d’une validation croisée \\(K\\) blocs pour le risque quadratique en régression, cette variance est approchée par\n\\[\n\\frac{1}{K}\\frac{1}{K-1}\\sum_{k=1}^K (\\mathcal R_k-\\mathcal R_{\\text{CV}})^2\n\\] où \\(\\mathcal R_k\\) représente l’erreur sur le bloc \\(k\\) et \\(\\mathcal R_{\\text{CV}}\\) est l’erreur de validation croisée. L’estimation de cette variance est très importante puisqu’elle permet d’avoir une idée sur les erreurs obtenues. Elle peut donc aider l’utilisateur à calibrer les algorithmes.\n\nExercice 3.1 (Variance de l’estimateur du risque) On reprend les données générées avec la fonction des chapitres précédents :\n\ndonnees &lt;- gen_class_bin2D(n=500,graine=12345,bayes=0.20)$donnees\n\net on considère les estimateurs du risque obtenus avec la validation croisée suivants :\n\nset.seed(12345)\ntune_spec &lt;- \n  nearest_neighbor(neighbors=tune(),weight_func=\"rectangular\") |&gt; \n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\") \nppv_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(Y ~ .)\n\ngrille_k &lt;- tibble(neighbors=1:100)\nre_ech_cv &lt;- vfold_cv(donnees,v=10)\n\nppv.cv &lt;- ppv_wf |&gt; \n  tune_grid(\n    resamples = re_ech_cv,\n    grid = grille_k,\n    metrics=metric_set(accuracy))\n\nOn visualise la courbe de risque avec\n\ntbl &lt;- ppv.cv |&gt; collect_metrics()\nggplot(tbl)+aes(x=neighbors,y=mean)+geom_line()+ylab(\"Accuracy\")\n\n\n\n\n\nppv.cv |&gt; collect_metrics()\n## # A tibble: 100 × 7\n##    neighbors .metric  .estimator  mean     n std_err .config\n##        &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n##  1         1 accuracy binary     0.618    10  0.0247 Prepro…\n##  2         2 accuracy binary     0.618    10  0.0247 Prepro…\n##  3         3 accuracy binary     0.672    10  0.0215 Prepro…\n##  4         4 accuracy binary     0.672    10  0.0215 Prepro…\n##  5         5 accuracy binary     0.69     10  0.0209 Prepro…\n##  6         6 accuracy binary     0.69     10  0.0209 Prepro…\n##  7         7 accuracy binary     0.704    10  0.0176 Prepro…\n##  8         8 accuracy binary     0.704    10  0.0176 Prepro…\n##  9         9 accuracy binary     0.696    10  0.0242 Prepro…\n## 10        10 accuracy binary     0.696    10  0.0242 Prepro…\n## # ℹ 90 more rows\n\n\nExpliquer à quoi correspond la colonne std_err du tibble tbl.\n\nIl s’agit de l’écart-type (estimé) de l’estimateur de l’accuracy pour chaque valeur de \\(k\\). Cette quantité permet de donner une indication sur la précision de l’estimateur.\n\nExpliquer la sortie de\n\n(k_one_std_err &lt;- ppv.cv |&gt; select_by_one_std_err(desc(neighbors)))\n## # A tibble: 1 × 9\n##   neighbors .metric  .estimator  mean     n std_err .config \n##       &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   \n## 1        84 accuracy binary     0.698    10  0.0241 Preproc…\n## # ℹ 2 more variables: .best &lt;dbl&gt;, .bound &lt;dbl&gt;\n\n\nCette fonction renvoie la valeur de \\(k\\) maximale (de manière à minimiser la complexité de l’algorithme) telle que le risque ne dépasse pas le meilleur risque à un écart-type près.\n\nVisualiser les écart-types estimés sur la courbe de risque ainsi que la valeur de \\(k\\) optimale et celle de la question précédente. On pourra par exemple tracer le graphe suivant :\n\n\n\n\n\n\nOn remarque que les variances sont élevées, beaucoup de valeurs de \\(k\\) se situe au dessus de la valeur optimale moins son écart-type."
  },
  {
    "objectID": "03-comp-risque.html#répéter-les-méthodes-de-ré-échantillonnage",
    "href": "03-comp-risque.html#répéter-les-méthodes-de-ré-échantillonnage",
    "title": "3  Compléments",
    "section": "3.2 Répéter les méthodes de ré-échantillonnage",
    "text": "3.2 Répéter les méthodes de ré-échantillonnage\nLes méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du ré-échantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec l’approche tidymodels, il suffit de construire les échantillons adéquat. Par exemple,\n\npour répéter 20 fois une validation hold out, on définit les blocs avec\n\nho_rep &lt;- mc_cv(donnees,prop=2/3,times = 20)\n\npour la validation croisée, on utilise\n\ncv_rep &lt;- vfold_cv(donnees, v = 10, repeats = 20)\n\n\nLe reste ne change pas, il faudra utiliser ces objets dans l’option resamples de tune_grid.\n\nExercice 3.2 (Validation croisée répétée) Refaire l’Exercice 3.1 avec une validation croisée répétée 20 fois. On comparera notamment la variance des estimateurs obtenus.\n\nOn calcule les erreurs par validation croisée répétée 20 fois.\n\n\nOn trace les accuracy et on visualise les erreurs d’estimations avec des barres verticales de hauteur 2 écart-types.\n\n\nOn remarque bien que les barres sont moins longues que pour la validation croisée répétée (voir exercice Exercice 3.1), les écart-types sont donc plus petits et les estimations plus précises."
  },
  {
    "objectID": "03-comp-risque.html#calcul-parallèle",
    "href": "03-comp-risque.html#calcul-parallèle",
    "title": "3  Compléments",
    "section": "3.3 Calcul parallèle",
    "text": "3.3 Calcul parallèle\nLes validations croisées (répétées) peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec tunegrid, on peut par exemple utiliser la librairie doParallel pour utiliser plusieurs cœurs de la machine. À titre d’illustration, nous comparons ci-dessous les temps de calcul pour faire une validation croisée répétée 20 fois. On commence sans calcul parallèle\n\ngrille_k1 &lt;- tibble(neighbors=c(1,11,21,31))\nt1 &lt;- system.time(\n  ppv_wf |&gt; tune_grid(\n  resamples = cv_rep,\n  grid = grille_k1))\n\n\nt1\n##    user  system elapsed \n## 146.115   5.331 152.338\n\nPour paralléliser on regarde tout d’abord la nombre de cœurs disponibles sur la machine\n\nlibrary(doParallel)\ndetectCores()\n## [1] 8\n\nOn ouvre les connexions sur chaque cœur\n\ncl &lt;- makePSOCKcluster(4)\nregisterDoParallel(cl)\n\net on lance la validation croisée, elle sera automatiquement parallélisée\n\nt2 &lt;- system.time(\n  ppv_wf |&gt; tune_grid(\n  resamples = cv_rep,\n  grid = grille_k1))\n\n\nt2\n##    user  system elapsed \n##   0.490   0.094  32.988\n\nLe temps de calcul a bien été diminué. Enfin on n’oublie pas de refermer les connexions\n\nstopCluster(cl)"
  },
  {
    "objectID": "04-lda.html#prise-en-main-lda-et-qda-sur-les-iris-de-fisher",
    "href": "04-lda.html#prise-en-main-lda-et-qda-sur-les-iris-de-fisher",
    "title": "4  Analyse discriminante linéaire",
    "section": "4.1 Prise en main : LDA et QDA sur les iris de Fisher",
    "text": "4.1 Prise en main : LDA et QDA sur les iris de Fisher\nOn considère les données sur les iris de Fisher.\n\ndata(iris)\n\n\nA l’aide de la fonction PCA du package FactoMineR, réaliser une ACP en utilisant comme variables actives les 4 variables quantitatives du jeu de données. On mettra la variable Species comme variable qualitative supplémentaire (option quali.sup).\nReprésenter le nuage des individus sur les 2 premiers axes de l’ACP en utilisant une couleur différente pour chaque espèce d’iris (option habillage).\nA l’aide de la fonction lda du package MASS, effectuer une analyse discriminante linéaire permettant d’expliquer l’espèce par les 4 autres variables explicatives.\nReprésenter le nuage des individus sur les deux premiers axes de l’analyse discriminante linéaire (en utilisant une couleur différente pour chaque espèce d’iris).\nRappeler comment sont obtenues les coordonnées des individus sur chaque axe. En déduire une interprétation de la position des individus.\n\nLes coodonnées se déduisent en projetant les observations sur les variables discriminantes. On peut récupérer ces dernières dans\n\n\non déduit que l’axe 1 oppose les individus ayant des grosses Sépales (à droite) à ceux ayant des grosses pétales (à gauche).\n\nComparer les représentations des questions 2 et 4.\n\nPour le problème considéré, la représentation de la question 4 parait plus pertinente. C’est normal dans la mesure où elle prend en compte les groupes des individus pour construire les axes (contrairement à l’ACP).\n\nExpliquer les sorties des commandes suivantes (mod.lda est l’objet construit avec la fonction lda).\n\nscore &lt;- predict(mod.lda)$x\nldahist(score[,1],iris[,5])    \nldahist(score[,2],iris[,5])    \n\n\nScore contient les coordonnées des projections des individus sur les axes de l’analyse discriminante. On obtient ensuite les histogrammes correspondants aux distributions des coordonnées pour chaque groupe. On visualise clairement que le premier axe est très discriminant. C’est renforcé par la sortie :\nProportion of trace: LD1 LD2 0.9912 0.0088\n\n\nqui restitue, en proportion, la valeur du coefficient de Rayleigh de chaque axe discriminant.\n\nExécuter et analyser les sorties de la commande\n\nmod.lda2 &lt;- lda(Species~.,data=iris,CV=TRUE)\n\n\nUne validation croisée est ici effectuée pour prédire les probabilités a posteriori ainsi que les groupes de chaque observation. On peut obtenir ces prévisions avec\n\nComparer, en terme d’erreur de prévision, les performances de LDA et QDA.\n\nIl suffit de confronter les valeurs prédites aux valeurs observées :\n\n\nOn peut retrouver ces résultats avec du dplyr :"
  },
  {
    "objectID": "04-lda.html#un-exemple-multi-classes",
    "href": "04-lda.html#un-exemple-multi-classes",
    "title": "4  Analyse discriminante linéaire",
    "section": "4.2 Un exemple multi-classes",
    "text": "4.2 Un exemple multi-classes\nOn considère les jeux de données Vowel (training et test) qui se trouvent à cet url. On peut les importer avec\n\ndapp &lt;- read_csv(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train\")[,-1]\ndtest &lt;- read_csv(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test\")[,-1]\n\n\nExpliquer le problème.\n\nIl s’agit d’expliquer la variable y qui admet 11 modalités par les 9 autres variables qui sont toutes quantitatives. On transforme donc la variable cible en facteur :\n\nEffectuer une analyse discriminante linéaire (uniquement avec les données d’apprentissage) et visualiser les individus sur les 2 premiers axes de l’analyse discriminante. On pourra utiliser predict.\n\nIl faut ici récupérer les coordonnées des individus sur chaque axe :\n\n\nOn peut ainsi obtenir le graphe demandé.\n\nLa fonction suivante permet de choisir les axes à visualiser, ainsi que les centres de gravité projetés des groupes.\n\nrepres_axes &lt;- function(prev,cdg,axe1=1,axe2=2){\n  cdg &lt;- prev %&gt;% group_by(y) %&gt;% summarise_all(mean)\n  nom1 &lt;- paste(\"LD\",as.character(axe1),sep=\"\")\n  nom2 &lt;- paste(\"LD\",as.character(axe2),sep=\"\")\n  ggplot(prev)+aes_string(x=as.name(nom1),y=as.name(nom2))+\ngeom_point(aes(color=y))+\ngeom_point(data=cdg,aes(color=y),shape=17,size=4)+\ntheme_classic()\n}\n\nÉtudier la pertinence des axes.\n\nLes groupes sont mieux séparés sur les premiers axes de l’analyse, ce qui est logique. On peut le vérifier en étudiant la sortie Proportion of trace de la fonction lda que l’on peut retrouver avec\n\nReprésenter les individus sur le premier plan factoriel de l’ACP, on utilisera une couleur différente pour chaque groupe. On pourra utiliser le package FactoMineR.\nComparer cette projection avec celle obtenue par l’analyse discriminante linéaire.\n\nLa projection de l’ACP semble moins séparer les groupes que l’analyse discriminante. Ca parait logique puisque l’ACP ne prend pas en compte les groupes pour construire ses axes.\n\nÉvaluer la performance de la lda sur les données test. Comparer avec l’analyse discriminante quadratique.\n\nQDA est légèrement plus performante pour le critère étudié.\n\nExpliquer comment on peut faire de la prévision en réduisant la dimension de l’espace des \\(X\\).\n\nAu lieu de projeter sur tous les axes, on considère uniquement les projections sur les \\(k\\) premiers axes de l’analyse discriminante. Cela permet de réduire la dimension et (éventuellement) de supprimer du bruit (qui se retrouve souvent sur les derniers axes). Cette métode s’appelle Reduced-rank discriminant analysis\n\nProposer une méthode permettant de choisir le meilleur nombre d’axes. On pourra notamment utiliser l’option dimen de la fonction predict.lda.\n\nCette option permet en effet d’utiliser uniquement les premiers axes de l’analyse discriminante. On peut ainsi estimer la probabilité d’erreur en fonction de la dimension de l’espace sur lequel on projette :\n\n\nOn remarque que l’erreur est minimale en projetant uniquement sur les deux premiers axes. Cette erreur est de plus comparable avec celle obtenue pour QDA."
  },
  {
    "objectID": "04-lda.html#grande-dimension-reconnaissance-de-phonèmes",
    "href": "04-lda.html#grande-dimension-reconnaissance-de-phonèmes",
    "title": "4  Analyse discriminante linéaire",
    "section": "4.3 Grande dimension : reconnaissance de phonèmes",
    "text": "4.3 Grande dimension : reconnaissance de phonèmes\nOn considère le jeu de données phoneme téléchargeable à l’url https://github.com/cran/ElemStatLearn/blob/master/data/phoneme.RData.\n\nload('data/phoneme.RData')\ndata(phoneme)\ndonnees &lt;- phoneme[,-c(258)]\n\n\nExpliquer le problème et représenter pour chaque groupe la courbe moyenne.\n\nIl s’agit d’expliquer un son parmi 5 à partir de courbes discrétisées en 256 points. On peut tracer la courbe moyenne pour chaque groupe avec :\n\nSéparer les données en un échantillon d’apprentissage de taille 3000 et un échantillon test de taille 1509.\nEffectuer une analyse discriminante linéaire et une analyse discriminante quadratique sur les données d’apprentissage uniquement. Évaluer les performances de ces deux approches sur les données test.\n\nOn calcule tout d’abord lda et qda sur les données d’apprentissage :\n\n\nOn prédit les individus de l’échantillon test pour en déduire l’erreur :\n\nQuels peuvent être les intérêts d’effectuer une analyse discriminante régularisée dans ce contexte ? Effectuer une telle analyse à l’aide de la fonction rda du package klaR.\n\nLe nombre de variables est élevé, par conséquent on se retrouve avec beaucoup de paramètres à estimer. De plus, les données étant fonctionnelles, on a de la corrélation entre les les variables explicatives (ce qui est rarement bon pour bien estimer). On fait la rda sur les données d’apprentissage :\n\n\nOn prédit le test et on compare avec les autres modèles :\n\nSélectionner les paramètres de régularisation à l’aide du package caret. Comparer le nouveau modèle aux précédents.\n\nOn définit deux grilles de longueur 5 pour les paramètres lambda et gamma, et on fait une validation croisée 5 blocs pour sélectionner les paramètres. On parallélise sur 4 cœurs pour réduire le temps de calcul.\n\n\nOn évalue et compare les performances :\n\n\nLa régularisation a permis d’améliorer les performances."
  },
  {
    "objectID": "04-lda.html#exercices",
    "href": "04-lda.html#exercices",
    "title": "4  Analyse discriminante linéaire",
    "section": "4.4 Exercices",
    "text": "4.4 Exercices\n\nExercice 4.1 (MV pour LDA) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R\\).\n\nQuels sont les paramètres à estimer dans le modèle d’analyse discriminante linéaire.\n\nLe modèle LDA suppose que \\(X|Y=k\\sim\\mathcal N(\\mu_k,\\sigma^2)\\). Il faut donc estimer \\(\\mu_0,\\mu_1,\\sigma^2\\) et les probabilités \\(\\pi_k=\\mathbf P(Y=k),j=0,1\\).\n\nCalculer la vraisemblance conditionnelle à \\(Y\\) et en déduire les estimateurs des paramètres des lois gaussiennes.\n\nOn désigne par \\(\\mathcal I_k,k=0,1\\) les indices des observations dans le groupe \\(k\\). On a alors \\[L_{X|Y}(x_1,\\dots,x_n)=\\prod_{i\\in\\mathcal I_0}f_0(x_i) \\prod_{i\\in\\mathcal I_1}f_1(x_i).\\] On obtient ainsi\n\\[\n  \\begin{aligned}\n  \\mathcal L_{X|Y}(x_1,\\dots,x_n)=- & (n_0+n_1)\\log(\\sqrt{2\\pi})-(n_0+n_1)\\log \\sqrt{\\sigma^2} \\\\\n  -&\\frac{1}{2}\\sum_{i\\in\\mathcal I_0}\n  \\frac{(x_i-\\mu_0)^2}{\\sigma^2}-\n  \\frac{1}{2}\\sum_{i\\in\\mathcal I_1}\n  \\frac{(x_i-\\mu_1)^2}{\\sigma^2}.\n\\end{aligned}\n\\]\nEn dérivant par rapport à \\(\\mu_0\\) et \\(\\mu_1\\) il est facile de voir que les EMV de ces paramètres sont donnés par\n\\[\\widehat\\mu_0=\\frac{1}{n_0}\\sum_{i\\in\\mathcal\nI_0}x_i\\quad\\text{et}\\quad\n\\widehat\\mu_1=\\frac{1}{n_1}\\sum_{i\\in\\mathcal I_1}x_i.\\]\nOn dérive maintenant par rapport à \\(\\sigma^2\\) :\n\\[\\frac{\\partial\\mathcal L_{X|Y}(x_1,\\dots,x_n)}{\\partial\\sigma^2}=-\\frac{n_0+n_1}{\\sigma^2}+\\sum_{i\\in\\mathcal I_0}\\frac{(x_i-\\mu_0)^2}{2\\sigma^4}+\\sum_{i\\in\\mathcal I_1}\\frac{(x_i-\\mu_1)^2}{2\\sigma^4}.\\]\nOn déduit que l’EMV de \\(\\sigma^2\\) est donné par\n\\[\\widehat\\sigma^2=\\frac{1}{n}\\left(\\sum_{i\\in\\mathcal I_0}(x_i-\\widehat\\mu_0)^2+\\sum_{i\\in\\mathcal I_1}(x_i-\\widehat\\mu_1)^2\\right).\\]\n\nComparer les estimateurs obtenus avec ceux du cours.\n\nCes estimateurs correspondent à ceux proposés dans le cours, à part pour \\(\\widehat\\sigma^2\\) où on divise par \\(n-2\\) au lieu de \\(n\\) pour débiaiser.\n\n\n\n\nExercice 4.2 (Fonctions linéaires discriminantes) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R^p\\).\n\nRappeler le modèle d’analyse discriminante linéaire.\n\nLe modèle LDA fait l’hypothèse que \\(X|Y=k\\) est un vecteur gaussien de loi \\(\\mathcal N(\\mu_k,\\Sigma)\\) avec \\(\\mu_k\\in\\mathbb R^p\\) et \\(\\Sigma\\) matrice symétrique définie positive \\(p\\times p\\).\n\nSoit \\(x\\in\\mathbb R^p\\) un nouvel individu. Montrer que la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise \\(\\mathbf P(Y=k|X=x)\\) est équivalente à la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise les fonctions linéaires discriminantes (on prendra soin de rappeler la définition des fonctions linéaires discriminantes).\n\nSous le modèle LDA, maximiser \\(\\mathbf P(Y=k|X=x)\\) est équivalent à maximiser \\(\\pi_kf_k(x)\\) où \\(f_k(x)\\) est la densité d’un vecteur gaussien \\(\\mathcal N(\\mu_k,\\Sigma)\\). Par conséquent cela revient à maximiser \\[-\\frac{1}{2}(x-\\mu_k)^t\\Sigma^{-1}(x-\\mu_k)+\\log(\\pi_k),\\] ou encore à chercher la fonction linéaire discriminante \\[\\delta_k(x)=x^t\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^t\\Sigma^{-1}\\mu_k+\\log\\pi_k.\\] qui prend la plus grande valeur.\n\n\n\n\nExercice 4.3 (Approche géométrique de la LDA) On considère un \\(n\\)-échantillon i.i.d. \\((x_1,y_1),\\dots,(x_n,y_n)\\) où \\(x_i\\) est à valeurs dans \\(\\mathbb R^2\\) et \\(y_i\\) dans \\(\\{0,1\\}\\). On cherche une droite vectorielle \\(a\\) telle que les projections de chaque groupe sur \\(a\\) soient séparées “au mieux”. Dit autrement, on cherche \\(a\\) telle que\n\nla distance entre les centres de gravité \\[g_0=\\frac{1}{\\mbox{card}\\{i:y_i=0\\}}\\sum_{i:y_i=0}x_i\\quad\\textrm{et}\\quad g_1=\\frac{1}{\\mbox{card}\\{i:y_i=1\\}}\\sum_{i:y_i=1}x_i\\] projetés sur \\(a\\) soit maximale (cette distance est appelée distance interclasse) ;\nla distance entre les projections des individus et leur centre de gravité soit minimale (distance interclasse).\n\nPour un vecteur \\(u\\) de \\(\\mathbb R^2\\), on désigne par \\(\\pi_a(u)\\) son projeté sur la droite engendrée par \\(a\\). Sans perte de généralité on supposera dans un premier temps que \\(a\\) est de norme 1.\n\nRappeler les définitions des variances totale \\(V\\), intra \\(W\\) et inter \\(B\\) des observations \\((x_1,y_1),\\dots,(x_n,y_n)\\).\n\nOn a \\[V=\\frac{1}{n}\\sum_{i=1}^n(x_i-g)(x_i-g)^t,\\ B=\\frac{1}{n}\\sum_{k=1}^Kn_k(g_k-g)(g_k-g)^t,\\] \\[W=\\frac{1}{n}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-g_k)(x_i-g_k)^t.\\]\n\nPour \\(u\\) fixé dans \\(\\mathbb R^2\\), exprimer \\(\\pi_a(u)\\) en fonction de \\(u\\) et \\(a\\) et en déduire que \\(\\|\\pi_a(u)\\|^2=a^tuu^ta\\).\n\nOn a \\[\\pi_a(u)=\\frac{\\langle u,a\\rangle}{\\|a\\|^2}a=u^ta a\\quad\\text{ si }\\quad\\|a\\|=1.\\] On déduit \\(\\|\\pi_a(u)\\|^2=a^tuu^ta\\).\n\nExprimer les variances totale \\(V(a)\\), intra \\(W(a)\\) et inter \\(B(a)\\) projetées sur \\(a\\) en fonction des variances calculées à la question 1.\n\nOn a ainsi \\[V(a)=\\frac{1}{n}\\sum_{i=1}^n\\|\\pi_a(x_i-g)\\|^2=\\frac{1}{n}\\sum_{i=1}^na^t(x_i-g)^t(x_i-g)a=a^tVa.\\] On montre de même que \\(B(a)=a^tBa\\) et \\(W(a)=a^tWa\\).\n\nOn cherche maintenant à maximiser\n\\[J(a)=\\frac{B(a)}{W(a)}\\]\nou encore à\n\\[\n\\textrm{maximiser }B(a)\\quad\\textrm{sous la contrainte}\\quad W(a)=1.\n\\tag{4.1}\\]\nLa méthode des multiplicateurs de Lagrange permet de résoudre un tel problème. La solution du problème de maximisation d’une fonction \\(f(x)\\) sujette à \\(h(x)=0\\) s’obtient en résolvant l’équation\n\\[\\frac{\\partial L(x,\\lambda)}{\\partial x}=0,\\quad\\textrm{où}\\quad L(x,\\lambda)=f(x)+\\lambda h(x).\\]\n\nMontrer que la solution du problème équation 4.1 est un vecteur propre de \\(W^{-1}B\\) associé à la plus grande valeur propre de \\(W^{-1}B\\). On note \\(a^\\star\\) cette solution.\n\nOn écrit le Lagrangien \\[L(a,\\lambda)=B(a)+\\lambda W(a)=a^t(B+\\lambda W)a,\\] puis on le dérive par rapport à \\(a\\) : \\[\\frac{\\partial L(a,\\lambda)}{\\partial a}=2(B+\\lambda W)a.\\] Par conséquent la solution \\(a^\\star\\) vérifie \\(W^{-1}Ba^\\star=\\lambda a^\\star\\), c’est donc un vecteur propre de \\(W^{-1}B\\). De plus, il est facile de voir que \\(J(a^\\star)=\\lambda\\), par conséquent \\(a^\\star\\) est un vecteur propre associé à la plus grande valeur propre de \\(W^{-1}B\\).\n\nMontrer que \\(a^\\star\\) est colinéaire à \\(W^{-1}(g_1-g_0)\\). On pourra admettre que, dans le cas de 2 groupes, on a \\[B=\\frac{n_0n_1}{n^2}(g_1-g_0)(g_1-g_0)^t.\\]\n\n\\(W^{-1}B\\) possédant au plus une valeur propre non nulle, il suffit de vérifier que \\(W^{-1}(g_1-g_0)\\) est vecteur propre de \\(W^{-1}B\\). On a \\[W^{-1}B W^{-1}(g_1-g_0)=\\frac{n_0n_1}{n^2} W^{-1}(g_1-g_0)(g_1-g_0)^t W^{-1}(g_1-g_0)=\\lambda W^{-1}(g_1-g_0)\\] avec \\[\\lambda=\\frac{n_0n_1}{n^2}(g_1-g_0)^t W^{-1}(g_1-g_0).\\]\n\nOn considère la règle géométrique d’affectation qui consiste à classer un nouvel individu \\(x\\in\\mathbb R^p\\) au groupe 1 si son projeté sur \\(a^\\star\\) est plus proche de \\(\\pi_{a^\\star}(g_1)\\) que de \\(\\pi_{a^\\star}(g_0)\\). Montrer que \\(x\\) sera affecté au groupe 1 si \\[S(x)=x^tW^{-1}(g_1-g_0)&gt;s\\] où on exprimera \\(s\\) en fonction de \\(g_0\\), \\(g_1\\) et \\(W\\).\n\nLe nouvel \\(x\\) est affecté au groupe \\(1\\) si \\(\\|\\pi_{a^\\star}(x-g_1)\\|\\leq \\|\\pi_{a^\\star}(x-g_0)\\|\\). Or \\[\\pi_{a^\\star}(x-g_1)=\\frac{(x-g_1)^ta_1^\\star}{\\|a_1^\\star\\|^2} a_1^\\star\\quad\\text{et}\\quad \\pi_{a^\\star}(x-g_0)=\\frac{(x-g_0)^ta_1^\\star}{\\|a_1^\\star\\|^2} a_1^\\star\\] où \\(a_1^\\star=W^{-1}(g_1-g_0)\\). Par conséquent, \\(x\\) est affecté au groupe 1 si \\[((x-g_1)^ta_1^\\star)^2\\leq ((x-g_0)^ta_1^\\star)^2\\Longleftrightarrow -2x^ta_1^\\star\\times g_1^ta_1^\\star+(g_1^ta_1^\\star)^2\\leq -2x^ta_1^\\star\\times g_0^ta_1^\\star+(g_0^ta_1^\\star)^2\\] ou encore \\[2x^ta_1^\\star(g_0^ta_1^\\star-g_1^ta_1^\\star)\\leq (g_0^ta_1^\\star)^2-(g_1^ta_1^\\star)^2=(g_0^ta_1^\\star+g_1^ta_1^\\star)(g_0^ta_1^\\star-g_1^ta_1^\\star).\\] Comme \\(g_0^ta_1^\\star-g_1^ta_1^\\star=-(g_1-g_0)^tW^{-1}(g_1-g_0)\\leq 0\\) on déduit que \\(x\\) est affecté au groupe 1 si \\[2x^ta_1^\\star\\geq g_0^ta_1^\\star+g_1^ta_1^\\star \\Longleftrightarrow x^tW^{-1}(g_1-g_0)\\geq \\frac{1}{2}(g_1+g_0)^tW^{-1}(g_1-g_0).\\]\n\nMontrer que cette règle est équivalente à choisir le groupe qui minimise la distance de Mahalanobis \\[d(x,g_k)=(x-g_k)^tW^{-1}(x-g_k),\\quad k=0,1.\\]\n\nIl est facile de voir que \\(d(x,g_1)\\leq d(x,g_0)\\) si et seulement si \\[x^tW^{-1}g_1-\\frac{1}{2}g_1^tW^{-1}g_1\\leq x^tW^{-1}g_0-\\frac{1}{2}g_0^tW^{-1}g_0.\\] D’où le résultat.\n\nOn revient maintenant à l’approche probabiliste de l’analyse discriminante linéaire vue en cours et on considère la règle d’affectation qui consiste à décider “groupe 1’’ si \\(\\mathbf P(Y=1|X=x)\\geq 0.5\\). Montrer que dans ce cas, un nouvel individu \\(x\\) est affecter au groupe 1 si :\n\\[\nS(x)=x^t\\Sigma^{-1}(\\mu_1-\\mu_0)&gt;\\frac{1}{2}(\\mu_1+\\mu_0)^t\\Sigma^{-1}(\\mu_1-\\mu_0)-\\log\\left(\\frac{\\pi_1}{\\pi_0}\\right).\n\\]\nConclure.\n\nSi on se place dans le modèle Gaussien d’analyse discriminante linéaire, \\(x\\) est affecté au groupe 1 si \\(\\mathbf P(Y=1|X=x)\\geq\\mathbf P(Y=0|X=x)\\), c’est-à-dire\n\\[\n\\log(\\pi_1)-\\frac{1}{2}(x-\\mu_1)^t\\Sigma^{-1}(x-\\mu_1)\\geq \\log(\\pi_0)-\\frac{1}{2}(x-\\mu_0)^t\\Sigma^{-1}(x-\\mu_0),\n\\]\nou encore\n\\[\nx^t\\Sigma^{-1}(\\mu_1-\\mu_0)\\geq \\frac{1}{2}(\\mu_1+\\mu_0)^t\\Sigma^{-1}(\\mu_1-\\mu_0)+\\log \\frac{\\pi_0}{\\pi_1}.\n\\]\nOn conclut en remarquant que si\n\n\\(\\mu_0\\) et \\(\\mu_1\\) sont estimés par \\(g_0\\) et \\(g_1\\) (ce qui est le cas généralement) ;\n\\(\\Sigma\\) est, à une constante multiplicative près, estimé par \\(W\\) (ce qui est le cas généralement) ;\nles probabilités a priori \\(\\pi_0\\) et \\(\\pi_1\\) sont égales,\n\nalors les 2 règles coïncident."
  },
  {
    "objectID": "05-svm.html#cas-séparable",
    "href": "05-svm.html#cas-séparable",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.1 Cas séparable",
    "text": "5.1 Cas séparable\nLe cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes :\n\\[\n\\min_{w,b}\\frac{1}{2}\\|w\\|^2\n\\tag{5.1}\\]\n\\[\n\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\n\\]\nLa solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\):\n\n\\(\\alpha_i^\\star=0\\)\n\nou\n\n\\(y_i(x_i^tw+b)-1=0.\\)\n\nCes conditions impliquent que \\(w^\\star\\) s’écrit comme une combinaison linéaire de quelques points, appelés vecteurs supports qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple.\nOn considère le nuage de points suivant :\n\nn &lt;- 20\nset.seed(123)\nX1 &lt;- scale(runif(n))[,1]\nset.seed(567)\nX2 &lt;- scale(runif(n))[,1]\nY &lt;- rep(-1,n)\nY[X1&gt;X2] &lt;- 1\nY &lt;- as.factor(Y)\ndonnees &lt;- tibble(X1,X2,Y)\np &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point()\np\n\n\n\n\nLa fonction svm du package e1071 permet d’ajuster une SVM :\n\nlibrary(e1071)\nmod.svm &lt;- svm(Y~.,data=donnees,kernel=\"linear\",cost=10000000000)\n\n\nRécupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un tiible dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports.\n\nLes vecteurs supports se trouvent dans la sortie index de la fonction svm :\n\n\nind.svm &lt;- mod.svm$index\nsv &lt;- donnees |&gt; slice(ind.svm)\n...\n\n\nOn peut ainsi représenter la marge en traçant les droites qui passent par ces points :\n\nRetrouver ce graphe à l’aide de la fonction plot.\nRappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm.\n\nUne fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star &gt;0}.\\]\nL’objet mod.svm$coefs contient les coefficients \\(\\alpha_i^\\star y_i\\) pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec\n\n\nL’hyperplan séparateur a donc pour équation : \\[-1.74x_1+2.12x_2-0.40=0.\\] On peut retrouver directement cette équation avec la fonction coef :\n\n\nOn peut donc rajouter la règle de décision :\n\nOn dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe.\n\nIl suffit de calculer \\(\\langle w^\\star,x\\rangle+b\\) et de prédire en fonction du signe de cette valeur :\n\n\nOn prédira le groupe -1 pour ce nouvel individu.\n\nRetrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE.\n\nPlus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score.\n\nObtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm.\n\nComme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes -1 et 1 à partir du score, il “suffit” de ramener ce score sur l’échelle \\([0,1]\\) avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores \\(S(x)\\) :\n\\[\nP(Y=1|X=x)=\\frac{1}{1+\\exp(aS(x)+b)}.\n\\]\nOn peut retrouver ces probabilités avec :"
  },
  {
    "objectID": "05-svm.html#cas-non-séparable",
    "href": "05-svm.html#cas-non-séparable",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.2 Cas non séparable",
    "text": "5.2 Cas non séparable\nDans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème équation 5.1. On va donc autoriser certains points à être :\n\nmal classés\n\net/ou\n\nbien classés mais à l’intérieur de la marge.\n\nMathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que :\n\n\\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ;\n\\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé.\n\nLe problème d’optimisation est alors de minimiser en \\((w,b,\\xi)\\)\n\\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes }\n\\left\\{\n  \\begin{array}{l}\ny_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\\n\\xi_i\\geq 0, i=1,\\dots,n.\n  \\end{array}\\right.\\]\nLe paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrit toujours comme une combinaison linéaire\n\\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\]\nde vecteurs supports sauf qu’on distingue deux types de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)):\n\nceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ;\nceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\).\n\nLe choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm :\n\n\\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports).\n\\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement.\n\nOn choisit généralement ce paramètre à l’aide des techniques présentées dans le chapitre Chapitre 2 :\n\nchoix d’une grille de valeurs de \\(C\\) et d’un critère ;\nchoix d’une méthode de ré-échantillonnage pour estimer le critère ;\nchoix de la valeur de \\(C\\) qui minimise le critère estimé.\n\nOn considère le jeu de données df3 définie ci-dessous.\n\nn &lt;- 1000\nset.seed(1234)\ndf &lt;- as.data.frame(matrix(runif(2*n),ncol=2))\ndf1 &lt;- df |&gt; filter(V1&lt;=V2)|&gt; mutate(Y=rbinom(n(),1,0.95))\ndf2 &lt;- df |&gt; filter(V1&gt;V2)|&gt; mutate(Y=rbinom(n(),1,0.05))\ndf3 &lt;- bind_rows(df1,df2) |&gt; mutate(Y=as.factor(Y))\nggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+\n  scale_color_manual(values=c(\"#FFFFC8\", \"#7D0025\"))+\n  theme(panel.background = element_rect(fill = \"#BFD5E3\", colour = \"#6D9EC1\",size = 2, linetype = \"solid\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nAjuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost.\n\nmod.svm1 &lt;- svm(Y~.,data=df3,kernel=\"linear\",...)\nmod.svm2 &lt;- svm(Y~.,data=df3,kernel=\"linear\",...)\nmod.svm3 &lt;- svm(Y~.,data=df3,kernel=\"linear\",...)\n\nCalculer les nombres de vecteurs supports pour chaque valeur de \\(C\\).\nVisualiser les 3 svm obtenues. Interpréter.\n\nPour \\(C\\) petit, toutes les observations sont classées 0, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque \\(C\\) augmente.\n\nSélectionner une valeur de \\(C\\) parmi les trois proposées. On pourra utiliser la fonction tune_grid.\n\nOn obtient les résultats avec"
  },
  {
    "objectID": "05-svm.html#lastuce-du-noyau",
    "href": "05-svm.html#lastuce-du-noyau",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.3 L’astuce du noyau",
    "text": "5.3 L’astuce du noyau\nLes SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à :\n\nplonger les données dans un nouvel espace appelé espace de représentation ou feature space ;\nappliquer une svm linéaire dans ce nouvel espace.\n\nLe terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le feature space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple\n\nLinéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x')=x^tx'\\).\nPolynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x')=(x^tx'+1)^d\\).\nGaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x')=\\exp\\left(-\\frac{\\|x-x'\\|}{2\\sigma^2}\\right).\\]\nLaplace (sur \\(\\mathbb R\\)) : \\(K(x,x')=\\exp(-\\gamma|x-x'|)\\).\nNoyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x')=\\min(x,x')\\).\n…\n\nBien entendu, en pratique tout le problème va consister à trouver le bon noyau !\nOn considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\).\n\nn &lt;- 500\nset.seed(13)\nX &lt;- matrix(runif(n*2,-2,2),ncol=2) |&gt; as.data.frame()\nY &lt;- rep(0,n)\ncond &lt;- (X$V1^2+X$V2^2)&lt;=2.8\nY[cond] &lt;- rbinom(sum(cond),1,0.9)\nY[!cond] &lt;- rbinom(sum(!cond),1,0.1)\ndf &lt;- X |&gt; mutate(Y=as.factor(Y))\nggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic()\n\n\n\n\n\nAjuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ?\n\nLa svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre).\n\nExécuter la commande suivante et commenter la sortie.\n\nmod.svm1 &lt;- svm(Y~.,data=df,kernel=\"radial\",gamma=1,cost=1)\nplot(mod.svm1,df,grid=250)\n\n\n\n\n\nLe noyau radial permet de mettre en évidence une séparation non linéaire.\n\nFaire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost).\n\nmod.svm2 &lt;- svm(Y~.,data=df,kernel=\"radial\",gamma=...,cost=...)\nmod.svm3 &lt;- svm(Y~.,data=df,kernel=\"radial\",gamma=...,cost=...)\nmod.svm4 &lt;- svm(Y~.,data=df,kernel=\"radial\",gamma=...,cost=...)\n\nplot(mod.svm2,df,grid=250)\nplot(mod.svm3,df,grid=250)\nplot(mod.svm4,df,grid=250)\n\nmod.svm2$nSV\nmod.svm3$nSV\nmod.svm4$nSV\n\n\nLe nombre de vecteurs supports diminue lorsque \\(C\\) augmente. Une forte valeur de \\(C\\) autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage).\n\nSélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4).\n\ntune.out &lt;- tune(svm,Y~.,data=...,kernel=\"...\",\n             ranges=list(cost=...,gamma=...))\n\n\nLa sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs.\n\nFaire de même en utilisant tidymodels\nVisualiser la règle sélectionnée.\n\nOn ré-entraîne l’algorithme sur toutes les données avec les paramètres calibrés :\n\n\nPour visualiser la règle, on extrait le modèle ajusté dans le workflow, puis on le visalise avec la fonction plot"
  },
  {
    "objectID": "05-svm.html#support-vector-regression",
    "href": "05-svm.html#support-vector-regression",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.4 Support vector regression",
    "text": "5.4 Support vector regression\nDans un contexte de régression (lorsque \\(y_i\\in\\mathbb R\\)), on ne recherche plus la l’hyperplan qui va séparer au mieux. On va dans ce cas là cherche à approcher au mieux les valeurs de \\(y_i\\). Cela revient à chercher \\(w\\in\\mathbb R^p\\) et \\(b\\in\\mathbb R\\) tels que\n\\[|\\langle w,x_i\\rangle+b-y_i|\\leq \\varepsilon\\]\navec \\(\\varepsilon&gt;0\\) petit à choisir par l’utilisateur. Par analogie avec la SVM binaire, on va ainsi chercher \\((w,b)\\) qui minimisent\n\\[\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } |y_i-\\langle w,x_i\\rangle-b|\\leq \\varepsilon,\\ i=1,\\dots,n,\\]\nLes contraintes impliquent que toute les observations doivent se définir dans une marge ou bande de taille \\(2\\varepsilon\\). Cette hypothèse peut amener l’utilisateur à utiliser des valeurs de \\(\\varepsilon\\) très grandes et empêcher la solution de bien ajuster le nuage de points. Pour pallier à cela, on introduit, comme dans le cas de la SVM binaire, des variables ressorts qui vont autoriser certaines observations à se situer en dehors de la marge. Le problème revient alors à trouver \\((w,b,\\xi,\\xi^\\star)\\) qui minimise\n\\[\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^n(\\xi_i+\\xi_i^\\star)\\] \\[\\textrm{sous les contraintes } \\left\\{\n      \\begin{array}{l}\n        y_i-\\langle w,x_i\\rangle-b\\leq \\varepsilon+\\xi_i,\\ i=1,\\dots,n,\\\\\n        \\langle w,x_i\\rangle+b-y_i\\leq \\varepsilon+\\xi_i^\\star,\\ i=1,\\dots,n \\\\\n        \\xi_i\\geq 0,\\xi_i^\\star\\geq 0,\\ i=1,\\dots,n\n      \\end{array}\\right.\n    \\] Les solutions s’obtiennent exactement de la même façon que dans le cas binaire. On montre notamment que \\(w^\\star\\) s’écrit comme une combinaison linéaire de vecteurs supports :\n\\[w^\\star=\\sum_{i=1}^n(\\alpha_i^\\star-\\alpha_i)x_i.\\]\nLes vecteurs supports sont les observations vérifiant \\(\\alpha_i^\\star-\\alpha_i\\neq 0\\). Ici encore il faudra calibrer le paramètre \\(C\\) et on pourra utiliser l’astuce du noyau.\nOn considère le nuage de points \\((x_i,y_i),i=1,\\dots,n\\) définie ci-dessous:\n\nset.seed(321)\nn &lt;- 30\nX &lt;- runif(n)\neps &lt;- rnorm(n,0,0.2)\nY &lt;- 1+X+eps\ndf &lt;- data.frame(X,Y)\np1 &lt;- ggplot(df)+aes(x=X,y=Y)+geom_point()\np1\n\n\n\n\nOn souhaite faire une SVR permettant de prédire \\(Y\\) par \\(X\\). On peut l’obtenir sur R toujours avec la fonction svm de e1071:\n\nsvr1 &lt;- svm(Y~.,data=df,kernel=\"linear\",epsilon=0.5,cost=100,scale=FALSE)\n\nOn choisit ici exceptionnellement de ne pas réduire les \\(X\\).\n\nÉcrire une fonction R qui, à partir d’un objet svm, calcule l’équation de la droite de la SVR. Cette fonction pourra également tracer cette doite ainsi que la marge.\nComparer la SVR précédente avec celle utilisant epsilon=0.6.\n\nLa droite traverse moins bien le nuage de points pour cette valeur de \\(\\varepsilon\\) qui semble trop grande.\n\nOn ajoute le point de coordonnées \\((0.05,3)\\) aux données. Discuter de la SVR pour ce nouveau jeu de données en utilisant plusieurs valeurs pour C et epsilon.\n\ndf1 &lt;- df |&gt; bind_rows(data.frame(X=0.05,Y=3))\n\n\nOn commence par faire grandir la valeur de epsilon pour que toutes les observations soient dans la marge.\n\n\nToutes les observations sont bien dans la marge mais la règle n’est clairement pas pertinente. Il est préférable d’autoriser certaines observations à se situer en dehors de la marge, par exemple :"
  },
  {
    "objectID": "05-svm.html#svm-sur-les-données-spam",
    "href": "05-svm.html#svm-sur-les-données-spam",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.5 SVM sur les données spam",
    "text": "5.5 SVM sur les données spam\nOn considère le jeu de données spam où le problème est d’expliquer la variable type par les autres.\n\nlibrary(kernlab)\ndata(spam)\nsummary(spam$type)\n## nonspam    spam \n##    2788    1813\n\nOn veut comparer plusieurs svm en utilisant le package kernlab. On pourra trouver un descriptif du package à cette adresse https://www.jstatsoft.org/article/view/v011i09.\n\nUtiliser la fonction ksvm pour faire une svm linéaire et une svm à noyau gaussien. On prendra comme paramètre 1 pour C et pour le paramètre du noyau gaussien.\n\nLa svm linéaire correspond au noyau polynomial avec des valeurs de paramètres particulières :\n\n\nPour le noyau gaussien, il suffit d’utiliser l’option kernel=\"rbfdot\" :\n\nÉvaluer la performance des 2 svm précédentes en calculant l’erreur de classification par validation croisée 5 blocs. Comparer ces deux algorithmes.\n\nIl suffit d’utiliser l’option cross dans ksvm.\n\n\nOn remarque que l’erreur de classification est plus faible pour la svm linéaire. De plus, il y a un gros écart entre l’erreur de prévision et l’erreur d’ajustement pour le noyau gaussien, il est fort possible que l’on soit en sur-apprentissage avec ces valeurs de paramètres.\n\nRefaire la svm à noyau gaussien avec l’option kpar='automatic'. Expliquer.\n\nLe paramètre du noyau est ici calibré à partir d’une heuristique. La valeur choisie semble pertinente puisque l’erreur de prévision a diminué et est maintenant proche de l’erreur d’ajustement.\n\nOn s’intéresse maintenant à l’AUC. À partir de validation croisée, sélectionner un noyau (linéaire ou gaussien) ainsi que des valeurs de paramètres associés au noyau, sans oublier le paramètre C. On pourra utiliser le package tidymodels et comparer le résultat obtenu à celui d’une forêt aléatoire.\n\nIl faut tout d’abord initier le workflow. On peut consulter la page https://www.tidymodels.org/find/parsnip/ pour identifier les identifiants des paramètres. On fait les choix suivants :\n\n\nOn définit ensuite des grilles et la méthode de ré-échantillonnage.\n\n\nOn compare à une forêt aléatoire avec les paramètres par défaut :\n\n\nLa forêt aléatoire est (légèrement) plus performante en terme d’AUC."
  },
  {
    "objectID": "05-svm.html#exercices",
    "href": "05-svm.html#exercices",
    "title": "5  Support Vector Machine (SVM)",
    "section": "5.6 Exercices",
    "text": "5.6 Exercices\n\nExercice 5.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables.\n\nSoit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\).\n\nSoit \\(x_0\\in\\mathcal H\\). La solution correspond à la norme du projeté orthogonal de \\(x-x_0\\) sur \\(\\mathcal H\\), elle est donc colinéaire à \\(w\\) (car \\(w\\) est normal à \\(\\mathcal H\\)) et s’écrit\n\\[\\frac{\\langle x-x_0,w\\rangle}{\\|w\\|^2}w=\\frac{\\langle x,w\\rangle}{\\|w\\|^2}w-\\frac{\\langle x_0,w\\rangle}{\\|w\\|^2}w,\\]\nComme \\(\\langle x_0,w\\rangle=-b\\), on déduit que, si \\(\\|w\\|=1\\), alors\n\\[d_{\\mathcal H}(x)=\\frac{|\\langle w,x\\rangle+b|}{\\|w\\|}=|\\langle w,x\\rangle+b|=(\\langle w,x\\rangle+b)y\\]\nsi \\(y=\\text{signe}(\\langle w,x\\rangle+b)\\).\n\nExpliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\]\n\nSi \\((w,b)\\) est un hyperplan séparateur sa marge vaut\n\\[\\min_{i=1,\\dots,n}y_i(\\langle w,x_i\\rangle+b).\\]\nLe problème proposé revient donc à chercher l’hyperplan :\n\nqui sépare les groupes ;\ntel que la distance entre les observations et lui soit maximale.\n\n\nMontrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\]\n\nIl suffit de poser comme contrainte \\(M=1/\\|w\\|\\).\n\nOn rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit\n\\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\]\nSi on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par\n\\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\]\net le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par\n\n\\(\\alpha_i^\\star\\geq 0\\).\n\\(g_i(u_{\\alpha^\\star})\\geq 0\\).\n\\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\).\n\n\nÉcrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations.\n\nLe lagrangien s’écrit\n\\[L(w,b;\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n\\alpha_i[y_i(\\langle w,x_i\\rangle+b)-1].\\] On a alors\n\\[\\frac{\\partial L(w,b;\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\] et\n\\[\\frac{\\partial L(w,b;\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_iy_i=0.\\]\nD’où \\(w_\\alpha=\\sum_{i=1}^n\\alpha_iy_ix_i\\).\n\nÉcrire la fonction duale.\n\nLa fonction duale s’écrit\n\\[\n\\begin{aligned}\n\\theta(\\alpha)=L(w_\\alpha,b_\\alpha;\\alpha)= & \\        \\frac{1}{2}\\langle \\sum_i\\alpha_iy_ix_i,\\sum_j\\alpha_jy_jx_j\\rangle-\\sum_i\\alpha_iy_i\\langle \\sum_j\\alpha_jy_jx_j,x_i\\rangle \\\\\n& -\\sum_i\\alpha_iy_ib+\\sum_i\\alpha_i \\\\\n= & \\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\langle x_i,x_j\\rangle\n\\end{aligned}\n\\]\nOn note \\(\\alpha_i^*\\) les valeurs de \\(\\alpha_i\\) qui maximisent \\(\\theta(\\alpha)\\).\n\nÉcrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\).\n\nOn peut déjà écrire\n\\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\]\nLes conditions KKT sont pour tout \\(i=1,\\dots,n\\) :\n\\[\\alpha_i^\\star\\geq 0 \\quad\\text{et}\\quad \\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0.\\]\nOn obtient \\(b^\\star\\) en résolvant\n\\[\\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0\\]\npour un \\(\\alpha_i^\\star\\) non nul.\n\nInterpréter les conditions KKT.\n\nLes \\(x_i\\) tels que \\(\\alpha_i^\\star&gt; 0\\) vérifient\n\\[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)=1.\\]\nIls se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports.\n\n\n\n\n\nExercice 5.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal)\n\\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\]\nOn désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus.\n\nDonner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\).\n\n\\(w^\\star\\) se calcule selon\n\\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\]\nLes \\(\\alpha_i^\\star\\) étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports.\n\nExpliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm.\n\nUne fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit\n\\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt; 0}.\\]\n\nLes données se trouvent dans un dataframe df. On exécute\n\nset.seed(1234)\nn &lt;- 100\nX &lt;- data.frame(X1=runif(n),X2=runif(n),X3=runif(n))\nX &lt;- data.frame(X1=scale(runif(n)),X2=scale(runif(n)),X3=scale(runif(n)))\nY &lt;- rep(-1,100)\nY[X[,1]&lt;X[,2]] &lt;- 1\n#Y &lt;- (apply(X,1,sum)&lt;=0) |&gt; as.numeric() |&gt; as.factor()\ndf &lt;- data.frame(X,Y=as.factor(Y))\n\n\nmod.svm &lt;- svm(Y~.,data=df,kernel=\"linear\",cost=10000000000)\n\net on obtient\n\ndf[mod.svm$index,]\n##      X1   X2   X3  Y\n## 51 -1.1 -1.0 -1.0  1\n## 92  0.7  0.8  1.1  1\n## 31  0.7  0.5 -1.0 -1\n## 37 -0.5 -0.6  0.3 -1\nmod.svm$coefs\n##      [,1]\n## [1,]   59\n## [2,]   49\n## [3,]  -30\n## [4,]  -79\nmod.svm$rho\n## [1] -0.5\n\nCalculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification.\n\n\\(b^\\star\\) est l’opposé de mod.svm$rho. Pour \\(w^\\star\\) il suffit d’appliquer la formule et on trouve\n\nOn dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ?\n\nOn calcule la combinaison linéaire \\(\\langle w^\\star,x\\rangle+b^\\star\\) :\n\n\nOn affectera donc la nouvelle donnée au groupe -1."
  },
  {
    "objectID": "06-arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables",
    "href": "06-arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables",
    "title": "6  Méthodes CART",
    "section": "6.1 Coupures CART en fonction de la nature des variables",
    "text": "6.1 Coupures CART en fonction de la nature des variables\nUne partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ?\nComme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de\n\ngrandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ;\nfaibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud).\n\nOn utilise souvent comme fonction d’impureté :\n\nla variance en régression\n\\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_{\\mathcal N})^2,\\] où \\(\\overline{y}_{\\mathcal N}\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\).\nl’impureté de Gini en classification binaire\n\\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\).\n\nLes coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où\n\n\\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ;\n\\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\).\n\n\n6.1.1 Arbres de régression\nOn considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\).\n\nn &lt;- 50\nset.seed(1234)\nX &lt;- runif(n)\nset.seed(5678)\nY &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1)\ndata1 &lt;- data.frame(X,Y)\nggplot(data1)+aes(x=X,y=Y)+geom_point()\n\n\n\n\n\nA l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\).\n\nlibrary(rpart)\n\nVisualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot.\n\nlibrary(rpart.plot)\n\nÉcrire l’estimateur associé à l’arbre.\n\nOn a un modèle de régression\n\\[Y=m(X)+\\varepsilon\\]\noù la fonction de régression (inconnue) \\(m(x)\\) est estimée par\n\\[\\widehat m(x)=0.31\\, \\mathbf{1}_{x\\lt 0.58}+2.4\\,\\mathbf{1}_{x\\geq 0.58}.\\]\n\nAjouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites.\n\nOn obtient une partition avec 2 nœuds terminaux. Cette partition peut être résumée par la question : “est-ce que \\(X\\) est plus petit que 0.58 ?”.\n\n\n\n\n6.1.2 Arbres de classification\nOn considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\).\n\nn &lt;- 50\nset.seed(12345)\nX1 &lt;- runif(n)\nset.seed(5678)\nX2 &lt;- runif(n)\nY &lt;- rep(0,n)\nset.seed(54321)\nY[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85)\nset.seed(52432)\nY[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15)\ndata2 &lt;- data.frame(X1,X2,Y)\nggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+\n  scale_x_continuous(name=\"\")+\n  scale_y_continuous(name=\"\")\n\n\n\n\n\nConstruire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème.\n\nOn observe que l’arbre construit est un arbre de régression, pas de classification. Cela vient du fait que \\(Y\\) est considérée par R comme une variable quantitative, il faut la convertir en facteur.\n\n\nTout est OK maintenant !\n\nÉcrire la règle de classification ainsi que la fonction de score définies par l’arbre.\n\nLa règle de classification est\n\\[\\widehat g(x)=\\mathbf{1}_{X_1\\lt 0.44}.\\]\nLa fonction de score est donnée par\n\\[\\widehat S(x)=\\widehat P(Y=1|X=x)=0.83\\mathbf{1}_{X_1\\lt 0.44}+0.07\\mathbf{1}_{X_1\\geq 0.44}.\\]\n\nAjouter sur le graphe de la question 1 la partition définie par l’arbre.\n\n\n\n6.1.3 Entrée qualitative\nOn considère les données\n\nn &lt;- 100\nX &lt;- factor(rep(c(\"A\",\"B\",\"C\",\"D\"),n))\nset.seed(1234)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),1,0.9)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),1,0.25)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),1,0.8)\nY[X==\"D\"] &lt;- rbinom(sum(X==\"D\"),1,0.2)\nY &lt;- as.factor(Y)\ndata3 &lt;- data.frame(X,Y)\n\n\nConstruire un arbre permettant d’expliquer \\(Y\\) par \\(X\\).\nExpliquer la manière dont l’arbre est construit dans ce cadre là.\n\nLa variable étant qualitative, on ne cherche pas un seuil de coupure pour diviser un nœud en 2. On va ici considérer toutes les partitions binaires de l’ensemble \\(\\{A,B,C,D\\}\\). La meilleure partition est \\(\\{\\{A,C\\},\\{B,D\\}\\}\\)."
  },
  {
    "objectID": "06-arbres.html#sec-elagage-cart",
    "href": "06-arbres.html#sec-elagage-cart",
    "title": "6  Méthodes CART",
    "section": "6.2 Élagage",
    "text": "6.2 Élagage\nLe procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important.\nLa méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes :\n\nOn construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ;\nOn sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre.\nOn sélectionne un arbre dans cette sous-suite en optimisant un critère de performance.\n\nCette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage.\n\n6.2.1 Élagage pour un problème de régression\nOn considère les données Carseats du package ISLR.\n\nlibrary(ISLR)\ndata(Carseats)\nsummary(Carseats)\n##      Sales          CompPrice       Income      \n##  Min.   : 0.000   Min.   : 77   Min.   : 21.00  \n##  1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75  \n##  Median : 7.490   Median :125   Median : 69.00  \n##  Mean   : 7.496   Mean   :125   Mean   : 68.66  \n##  3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00  \n##  Max.   :16.270   Max.   :175   Max.   :120.00  \n##   Advertising       Population        Price      \n##  Min.   : 0.000   Min.   : 10.0   Min.   : 24.0  \n##  1st Qu.: 0.000   1st Qu.:139.0   1st Qu.:100.0  \n##  Median : 5.000   Median :272.0   Median :117.0  \n##  Mean   : 6.635   Mean   :264.8   Mean   :115.8  \n##  3rd Qu.:12.000   3rd Qu.:398.5   3rd Qu.:131.0  \n##  Max.   :29.000   Max.   :509.0   Max.   :191.0  \n##   ShelveLoc        Age          Education    Urban    \n##  Bad   : 96   Min.   :25.00   Min.   :10.0   No :118  \n##  Good  : 85   1st Qu.:39.75   1st Qu.:12.0   Yes:282  \n##  Medium:219   Median :54.50   Median :14.0            \n##               Mean   :53.32   Mean   :13.9            \n##               3rd Qu.:66.00   3rd Qu.:16.0            \n##               Max.   :80.00   Max.   :18.0            \n##    US     \n##  No :142  \n##  Yes:258  \n##           \n##           \n##           \n## \n\nOn cherche ici à expliquer la variable quantitative Sales par les autres variables.\n\nConstruire un arbre permettant de répondre au problème.\nExpliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error.\n\nOn peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :\n\nCP : le paramètre de complexité, plus il est petit plus l’arbre est profond ;\nnsplit : nombre de coupures de l’arbre ;\nrel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;\nxerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;\nxstd correspond à l’écart type estimé de l’erreur.\n\nLes types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de régression, c’est l’erreur quadratique moyenne qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec\n\nConstruire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart.\n\nIl suffit de diminuer les valeurs par défaut de ces paramètres.\n\n\nOn obtient ici une suite de près de 300 arbres. On remarque que\n\nl’erreur d’ajustement ne cesse de décroître, ceci est logique vu le procédé de construction : on ajuste de mieux en mieux lorsqu’on augmente le nombre de coupures ;\nl’erreur de prévision décroit avant de d’augmenter à nouveau. C’est le phénomène bien connu du sur-apprentissage.\n\n\nExpliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente.\n\nOn obtient un graphe qui permet de visualiser l’erreur quadratique calculée par validation croisée (erreur de prévision) en fonction du paramètre cp ou nsplit.\n\nSélectionner le “meilleur” arbre dans la suite construite.\n\nLa manière classique revient à choisir l’arbre qui a la plus petite erreur de prévision. Cela revient à aller chercher dans le tableau de la fonction printcp l’arbre qiu possède la plus petite erreur de prévision. On peut obtenir la valeur optimale de cp avec\n\nVisualiser l’arbre choisi (utiliser la fonction prune).\n\nLa fonction visTree du package visNetwork permet de donner une visualisation interactive de l’arbre.\n\n\nUne application Shiny est également proposée pour visualiser les arbres\n\nOn souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus :\n\n(new_ind &lt;- Carseats |&gt; as_tibble() |&gt; \n   slice(3,58,185,218) |&gt; dplyr::select(-Sales))\n## # A tibble: 4 × 10\n##   CompPrice Income Advertising Population Price ShelveLoc\n##       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n## 1       113     35          10        269    80 Medium   \n## 2        93     91           0         22   117 Bad      \n## 3       132     33           7         35    97 Medium   \n## 4       106     44           0        481   111 Medium   \n## # ℹ 4 more variables: Age &lt;dbl&gt;, Education &lt;dbl&gt;,\n## #   Urban &lt;fct&gt;, US &lt;fct&gt;\n\nCalculer les valeurs prédites.\nSéparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150.\nOn considère la suite d’arbres définie par\n\nset.seed(4321)\ntree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2)\n\nDans cette suite, sélectionner\n\nun arbre très simple (avec 2 ou 3 coupures)\nun arbre très grand\nl’arbre optimal (avec la procédure d’élagage classique).\n\nCalculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test.\n\nPour chaque arbre \\(T\\) on calcule \\[\\frac{1}{n_\\text{test}}\\sum_{i\\in \\text{test}}(Y_i-T(X_i))^2.\\]\nOn définit une table qui regroupe les prédictions des 3 arbres sur l’échantillon test :\n\n\nOn en déduit les erreurs quadratique\n\n\nL’arbre sélectionné a ici la plus petite erreur.\n\nRefaire la comparaison avec une validation croisée 10 blocs.\n\nOn créé tout d’abord les blocs.\n\n\nOn fait la validation croisée.\n\n\n\n\n6.2.2 Élagage en classification binaire et matrice de coût\nOn considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous.\n\nHigh &lt;- ifelse(Carseats$Sales&lt;=8,\"No\",\"Yes\")\ndata1 &lt;- Carseats |&gt; dplyr::select(-Sales) |&gt; mutate(High)\n\n\nConstruire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente.\n\nL’arbre construit est un arbre de classification. Le procédé de découpe des noeuds est différent : il utilise l’impureté de Gini au lieu de la variance.\n\nExpliquer l’option parms dans la commande :\n\ntree1 &lt;- rpart(High~.,data=data1,parms=list(split=\"information\"))\ntree1$parms\n## $prior\n##    1    2 \n## 0.59 0.41 \n## \n## $loss\n##      [,1] [,2]\n## [1,]    0    1\n## [2,]    1    0\n## \n## $split\n## [1] 2\n\n\nOn change de fonction d’impureté (information au lieu de Gini).\n\nExpliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error.\n\nOn peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :\n\nCP : le paramètre de complexité, plus il est petit plus l’arbre est profond ;\nnsplit : nombre de coupures de l’arbre ;\nrel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;\nxerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;\nxstd correspond à l’écart type estimé de l’erreur.\n\nLes types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de classification, c’est l’erreur de classification qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec\n\nSélectionner un arbre optimal dans la suite.\nOn considère la suite d’arbres\n\ntree2 &lt;- rpart(High~.,data=data1,\n               parms=list(loss=matrix(c(0,5,1,0),ncol=2)),\n               cp=0.01,minsplit=2)\n\nExpliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable.\n\ntree2$parms\n## $prior\n##    1    2 \n## 0.59 0.41 \n## \n## $loss\n##      [,1] [,2]\n## [1,]    0    1\n## [2,]    5    0\n## \n## $split\n## [1] 1\nprintcp(tree2)\n## \n## Classification tree:\n## rpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, \n##     5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2)\n## \n## Variables actually used in tree construction:\n## [1] Advertising Age         CompPrice   Education  \n## [5] Income      Population  Price       ShelveLoc  \n## \n## Root node error: 236/400 = 0.59\n## \n## n= 400 \n## \n##          CP nsplit rel error xerror    xstd\n## 1  0.101695      0   1.00000 5.0000 0.20840\n## 2  0.050847      2   0.79661 3.8136 0.20909\n## 3  0.036017      3   0.74576 3.2034 0.20176\n## 4  0.035311      5   0.67373 3.1271 0.20038\n## 5  0.025424      9   0.50847 2.6144 0.19069\n## 6  0.016949     11   0.45763 2.3475 0.18307\n## 7  0.015537     16   0.37288 2.1992 0.17905\n## 8  0.014831     21   0.28814 2.1992 0.17905\n## 9  0.010593     23   0.25847 2.0466 0.17367\n## 10 0.010000     25   0.23729 2.0297 0.17292\n\n\nLe critère est ici modifié, on utilise une erreur de classification pondérée pour choisir l’arbre. On rappelle que l’erreur de classification est définie par \\[L(g)=P(g(X)\\neq Y)=E[\\alpha_11_{g(X)=0,Y=1}+\\alpha_21_{g(X)=1,Y=0}]\\] avec \\(\\alpha_1=\\alpha_2=1\\). Cette erreur donne donc le même poids aux deux erreurs possible (prédire 1 à tort ou prédire 0 à tort). Utiliser cette erreur revient donc à supposer qu’elles ont la même importance pour le problème considéré. Ce n’est bien entendu pas toujours le cas en pratique. La matrice loss contient les valeurs de \\(\\alpha_1\\) et \\(\\alpha_2\\) et modifier ces valeurs permettra de donner des poids différents à ces deux erreurs.\nAvec cette nouvelle commande, on donne un poids de 5 pour une erreur et de 1 pour l’autre. On obtient le terme demandé avec\n\nComparer les valeurs ajustées par les deux arbres considérés.\n\nIl y a plus de Yes prédits dans le second arbre. Cela vient des changements dans la matrice loss : la perte pour prédire No au lieu de Yes est de 5 pour le second arbre. Cela signifie bien détecter les Yes est plus important pour cet arbre, c’est donc tout à fait normal qu’il prédise plus souvent Yes que le premier.\nCette stratégie de changer la matrice de coût peut se révéler intéressante dans le cas de données déséquilibrées : une modalité de la cible sous-représentée par rapport à l’autre. En effet, pour de tels problèmes il est souvent très important de bien détecter la modalité sous-représentée. On pourra donc donner un poids plus fort lorsqu’on détecte mal cette modalité.\n\n\n\n\n6.2.3 Calcul de la sous-suite d’arbres optimaux\n\nExercice 6.1 (Minimisation du critère coût/complexité) On considère l’algorithme qui permet de calculer les suites \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\) du théorème présenté en cours. Pour simplifier on se place en classification binaire et on considère les notations suivantes (en plus de celles présentées dans le chapitre) :\n\n\\(R(t)\\) : erreur de classification dans le nœud \\(t\\) pondérée par la proportion d’individus dans le nœud (nombre d’individus dans \\(t\\) sue le nombre total d’individus).\n\\(T^t\\) : la branche de l’arbre \\(T\\) issue du nœud interne \\(t\\).\n\\(R(T^t)\\) : l’erreur de la branche \\(T^t\\) pondérée par la proportion d’individus dans le nœud.\n\nL’algorithme suivant présente le calcul explicite des suites \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\).\n\nInitialisation : on pose \\(\\alpha_0=0\\) et on calcule l’arbre maximale \\(T_0\\) qui minimise \\(C_0(T)\\). On fixe \\(m=0\\). Répéter jusqu’à obtenir l’arbre racine\n\nCalculer pour tous les nœuds \\(t\\) internes de \\(T_{\\alpha_m}\\) \\[g(t)=\\frac{R(t)-R(T_{\\alpha_m}^t)}{|{T_{\\alpha_m}^t}|-1}\\]\nChoisir le nœud interne \\(t_m\\) qui minimise \\(g(t)\\).\nOn pose \\[\\alpha_{m+1}=g(t_m)\\quad\\text{et}\\quad T_{\\alpha_{m+1}}=T_{\\alpha_m}-T_{\\alpha_m}^{t_m}.\\]\nMise à jour : \\(m:=m+1\\).\n\nRetourner : les suites finies \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\).\n\nOn propose d’utiliser cet algorithme sur l’arbre construit suivant\n\ngen_class_bin2D &lt;- function(n=100,graine=1234,bayes=0.1){\n  set.seed(graine)\n  grille &lt;- 0.1\n  X1 &lt;- runif(n)\n  X2 &lt;- runif(n)\n  Y &lt;- rep(0,n)\n  cond0 &lt;- (X1&gt;0.2 & X2&gt;=0.8) | (X1&gt;0.6 & X2&lt;0.4) | (X1&lt;0.25 & X2&lt;0.5)\n  cond1 &lt;- !cond0\n  Y[cond0] &lt;- rbinom(sum(cond0),1,bayes)\n  Y[cond1] &lt;- rbinom(sum(cond1),1,1-bayes)\n  donnees &lt;- tibble(X1,X2,Y=as.factor(Y))\n  px1 &lt;- seq(0,1,by=grille)\n  px2 &lt;- seq(0,1,by=grille)\n  px &lt;- expand.grid(X1=px1,X2=px2)\n  py &lt;- rep(0,nrow(px))\n  cond0 &lt;- (px[,1]&gt;0.2 & px[,2]&gt;=0.8) | \n    (px[,1]&gt;0.6 & px[,2]&lt;0.4) | \n    (px[,1]&lt;0.25 & px[,2]&lt;0.5)\n  cond1 &lt;- !cond0\n  py[cond0] &lt;- 0\n  py[cond1] &lt;- 1\n  df &lt;- px |&gt; as_tibble() |&gt; mutate(Y=as.factor(py))\n  p &lt;- ggplot(df)+aes(x=X1,y=X2,fill=Y)+\n    geom_raster(hjust=1,vjust=1)\n  return(list(donnees=donnees,graphe=p))\n}\ndon.2D.arbre &lt;- gen_class_bin2D(n=150,graine=3210,bayes=0.05)$donnees\nset.seed(123)\nT0 &lt;- rpart(Y~.,data=don.2D.arbre)\nrpart.plot(T0,extra = 1)\n\n\n\n\nL’arbre \\(T_0\\).\n\n\n\n\nCet arbre n’est pas l’arbre maximal mais la manière d’élaguer est identique.\n\nCalculer pour les 5 nœuds internes de \\(T_0\\) la fonction \\(g(t)\\).\n\nOn numérote les nœuds internes de haut en bas et de gauche à droite. On commence par le nœud \\(t_5\\), celui qui correspond à la coupure X1&gt;=0.31. On a \\[R(t_5)=\\frac{4}{23}\\ \\frac{23}{150}=\\frac{4}{150}\\quad\\text{et}\\quad R(T_0^{t_5})=\\frac{3}{23}\\ \\frac{23}{150}=\\frac{3}{150}.\\] On déduit \\[g(t_5)=\\frac{4/150-3/150}{2-1}=\\frac{1}{150}.\\] On fait de même pour les 4 autres nœuds internes et on obtient les résultats suivants :\n\n\n\nt\n\\(t_1\\)\n\\(t_2\\)\n\\(t_3\\)\n\\(t_4\\)\n\\(t_5\\)\n\n\n\n\n\\(R(t)\\)\n65/150\n20/150\n22/150\n12/150\n4/150\n\n\n\\(R(T_0^{t})\\)\n8/150\n2/150\n6/150\n1/150\n3/150\n\n\n\\(|{T_0^t}|\\)\n6\n3\n3\n2\n2\n\n\n\\(g(t)\\)\n11.4/150\n9/150\n8/150\n11/150\n1/150\n\n\n\n\nEn déduire la valeur de \\(\\alpha_1\\) ainsi que l’arbre \\(T_{\\alpha_1}\\).\n\n\\(g(t)\\) est minimum en \\(t_5\\) On a donc \\(\\alpha_1=g(t_5)\\) et \\(T_{\\alpha_1}=T_0-T_0^{t_5}\\), c’est-à-dire \\(T_0\\) auquel on enlève la coupure X1&gt;=0.31.\n\nRetrouver cette valeur en utilisant la fonction printcp et représenter l’arbre \\(T_1\\) en utilisant prune.\n\nOn se rappelle que printcp normalise toutes les erreurs par rapport à celle de l’arbre racine, par conséquent la valeur de \\(\\alpha_1\\) affichée par printcp sera \\[\\frac{1}{150} \\, \\frac{150}{65}=\\frac{1}{65}\\approx 0.01538.\\] En effet :\n\n\nEt on peut visualiser l’arbre avec\n\nFaire le même travail pour calculer \\(\\alpha_2\\) et \\(T_{\\alpha_2}\\).\n\nOn se place maintenant dans \\(T_{\\alpha_1}\\) qui contient 4 nœuds internes et on calcule \\(g(t)\\) pour ces 4 nœuds :\n\n\n\nt\n\\(t_1\\)\n\\(t_2\\)\n\\(t_3\\)\n\\(t_4\\)\n\n\n\n\n\\(R(t)\\)\n65/150\n20/150\n22/150\n12/150\n\n\n\\(R(T_{\\alpha_1}^t)\\)\n9/150\n18/150\n7/150\n1/150\n\n\n\\(|{T_{\\alpha_1}^t}|\\)\n5\n3\n2\n2\n\n\n\\(g(t)\\)\n14/150\n9/150\n15/150\n11/150\n\n\n\nOn supprimera ici \\(t_2\\) avec on posera \\(\\alpha_2=9/65\\approx 0.13846\\) (on normalise). On peut tracer \\(T_{\\alpha_2}\\) :\n\n\n\n\n\n\n\nBreiman, L., J. Friedman, R. Olshen, et C. Stone. 1984. Classification and regression trees. Wadsworth & Brooks."
  },
  {
    "objectID": "07-forets.html",
    "href": "07-forets.html",
    "title": "7  Forêts aléatoires",
    "section": "",
    "text": "Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. Dans ce chapitre on étudiera ces algorithmes sur le je de données spam :\n\nlibrary(kernlab)\ndata(spam)\nset.seed(1234)\nspam &lt;- spam[sample(nrow(spam)),]\n\nLe problème est d’expliquer la variable binaire type par les autres.\nL’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante :\n\nEntrées :\n\n\\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ;\n\\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud\n\\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud.\n\nAlgorithme : pour \\(k=1,\\dots,B\\) :\n\nTirer un échantillon bootstrap dans \\(\\mathcal D_n\\)\nConstruire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit.\n\nSortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\).\n\nCet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest ou la fonction ranger du package ranger.\n\nExercice 7.1 (Biais et variance des algorithmes bagging) Comparer le biais et la variance de la forêt \\(T_B(x)\\) au biais et à la variance d’un arbre de la forêt \\(T(x,\\theta_k,\\mathcal D_n)\\). On pourra utiliser \\(\\rho(x)=\\text{corr}(T(x,\\theta_1,\\mathcal D_n),T(x,\\theta_2,\\mathcal D_n))\\) pour comparer les variances.\n\nPour simplifier les notations on considère \\(T_1,\\dots,T_B\\) \\(B\\) variables aléatoires de même loi et de variance \\(\\sigma^2\\). Il est facile de voir que \\(\\mathbf E[\\bar T]=\\mathbf E[T_1]\\). Pour la variance on a\n\\[\n\\begin{aligned}\n\\mathbf V[\\bar T]= & \\frac{1}{B^2}\\mathbf V\\left[\\sum_{i=1}^BT_i\\right]= \\frac{1}{B^2}\\left[\\sum_{i=1}^V\\mathbf V[T_i]+\\sum_{i\\neq j}\\mathbf{cov}(T_i,T_j)\\right] \\\\\n= & \\frac{1}{B^2}\\left[B\\sigma^2+B(B-1)\\rho\\sigma^2\\right]=\\rho\\sigma^2+\\frac{1-\\rho}{B}\\sigma^2.\n\\end{aligned}\n\\]\nConsidérons \\(\\rho\\leq 0\\). On déduit de l’équation précédente que \\(B\\leq 1-1/\\rho\\). Par exemple si \\(\\rho=-1\\), \\(B\\) doit être inférieur ou égal à 2. Il n’est en effet pas possible de considérer 3 variables aléatoires de même loi dont les corrélations 2 à 2 sont égales à -1. De même si \\(\\rho=-1/2\\), \\(B\\leq 3\\)…\n\n\n\nExercice 7.2 (RandomForest versus ranger) On sépare le jeu de données spam en un échantillon d’apprentissage et un échantillon test :\n\nset.seed(1234)\nlibrary(tidymodels)\ndata_split &lt;- initial_split(spam, prop = 3/4)\nspam.app &lt;- training(data_split)\nspam.test  &lt;- testing(data_split)\n\n\nEntraîner une forêt aléatoire sur les données d’apprentissage uniquement en utilisant les paramètres par défaut de la fonction randomForest. Commenter.\n\nIl s’agit d’une forêt de classification avec 500 arbres. Le paramètre mtry vaut 7 et l’erreur OOB est de 4.72%.\n\nCalculer les groupes prédits pour les individus de l’échantillon test et en déduire une estimation de l’erreur de classification.\nCalculer les estimations de la probabilité de spam pour les individus de l’échantillon test.\nRefaire les questions précédentes avec la fonction ranger du package ranger (voir https://arxiv.org/pdf/1508.04409.pdf).\n\nlibrary(ranger)\n\n\nSi on souhaite estimer les probabilités d’être (ou pas) spam, il faut le spécifier dans la construction de la forêt :\n\nComparer les temps de calcul de randomForest et ranger.\n\nranger est beaucoup plus rapide.\n\n\n\n\nExercice 7.3 (Sélection des paramètres) Nous nous intéressons ici au choix des paramètres de la forêt aléatoire.\n\nExpliquer la sortie suivante.\n\nset.seed(12345)\nlibrary(OOBCurve)\nforet1 &lt;- ranger(type~.,data=spam,keep.inbag=TRUE)\nspam.task &lt;- mlr::makeClassifTask(data=spam,target=\"type\")\nerreurs &lt;- OOBCurve(foret1,measures = list(mmce, auc),\n                task=spam.task,data=spam)\nerreurs1 &lt;- erreurs |&gt; as_tibble() |&gt; mutate(ntrees=1:500) |&gt; \n  filter(ntrees&gt;=5) |&gt;\n  pivot_longer(-ntrees,names_to=\"Erreur\",values_to=\"valeur\")  \nggplot(erreurs1)+aes(x=ntrees,y=valeur)+geom_line()+\n  facet_wrap(~Erreur,scales=\"free\")\n\n\n\n\n\nCe graphe permet de visualiser l’évolution des erreurs OOB (AUC et erreur de classification) en fonction du nombre d’arbres. Il peut être utilisé pour voir si l’algorithme a bien “convergé”. Si ce n’est pas le cas, il faut construire une forêt avec plus d’arbres.\n\nConstruire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment.\n\nLa forêt foret1 est plus performante en terme d’erreur de classification OOB.\n\nA l’aide des outils tidymodels sélectionner les paramètres mtry et min_n dans les grilles c(1,6,seq(10,50,by=10),57) et c(1,5,100,500). On pourra notamment visualiser les critères en fonction des valeurs de paramètres.\n\nOn commence par construire la grille :\n\n\nOn définit le workflow\n\n\nOn effectue la validation croisée en parallélisant :\n\n\nOn étudie les meilleures valeurs de paramètres pour les deux critères considérés :\n\n\nOn visualise les erreurs de classification\n\n\net les AUC\n\n\nOn retrouve bien des petites valeurs pour min_n : il faut des arbres profonds pour que la forêt soit performante. Les valeurs optimales de mtry se situent autours de la valeur par défaut (7 ici). On peut donc conserver cette valeur pour ré-ajuster la forêt sur toutes les données :\n\nVisualiser l’importance des variables pour les scores d’impureté et de permutations.\n\n\n\nExercice 7.4 (Arbre vs forêt aléatoire) Proposer et mettre en œuvre une procédure permettant de comparer les performances (courbes ROC, AUC et accuracy) d’un arbre CART utilisant la procédure d’élagage proposée dans la Section 6.2 avec une forêt aléatoire.\n\nOn peut envisager différentes stratégies pour répondre à cette question. Il convient de bien préciser ce que l’on souhaite faire. Il ne s’agit pas de sélectionner les paramètres d’un algorithme. On souhaite comparer deux algorithmes de prévision :\n\nun arbre CART qui utilise la procédure d’élagage CART : création de la suite optimale de sous arbre puis sélection d’un arbre dans cette suite en estimant l’erreur de classification par validation croisée ;\nune forêt aléatoire qui prend les valeurs par défaut pour nodesize et qui sélection mtry en minimisant l’erreur OOB (c’est un choix).\n\nIl faut estimer les risques demandés en se donnant une stratégie de ré-échantillonnage. On choisit une validation croisée 10 blocs :\n\n\nOn crée une fonction spécifique à chaque algorithme qui calculera les prévisions de nouveaux individus :\n\n\nOn effectue la validation croisée :\n\n\nOn déduit la courbe ROC, l’AUC\n\n\net l’accuracy\n\n\n\n\n\n\nBreiman, L. 2001. « Random forests ». Machine learning 45: 5‑32.\n\n\nFriedman, J. H. 2001. « Greedy Function Approximation: A Gradient Boosting Machine ». Annals of Statistics 29: 1189‑1232."
  },
  {
    "objectID": "08-boosting.html#sec-sinusgbm",
    "href": "08-boosting.html#sec-sinusgbm",
    "title": "8  Gradient boosting",
    "section": "8.1 Un exemple simple en régression",
    "text": "8.1 Un exemple simple en régression\nOn considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas).\n\nx &lt;- seq(-2*pi,2*pi,by=0.01)\ny &lt;- sin(x)\nset.seed(1234)\nX &lt;- runif(200,-2*pi,2*pi)\nY &lt;- sin(X)+rnorm(200,sd=0.2)\ndf1 &lt;- data.frame(X,Y)\ndf2 &lt;- data.frame(X=x,Y=y)\np1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+\n  geom_line(data=df2,linewidth=1)+xlab(\"\")+ylab(\"\")\np1\n\n\n\n\n\nRappeler ce que siginifie le \\(L_2\\)-boosting.\n\nIl s’agit de l’algorithme de gradient boosting présenté ci-dessus appliqué à la fonction de perte \\[\\ell(y,f(x))=\\frac{1}{2}(y-f(x))^2.\\]\n\nA l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres, à l’exception de bag.fraction qu’on prendra égal à 1.\n\nlibrary(gbm)\nL2boost &lt;- gbm(Y~.,data=df1,...)\n\nVisualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees ou utiliser directement la fonction plot.gbm avec l’option n.trees.\n\nhelp(predict.gbm)\nprev1 &lt;- predict(L2boost,...)\n#ou\nhelp(plot.gbm)\nplot(L2boost,...)\n\n\nOn remarque que l’estimateur est un arbre avec une seule coupure. On aurait aussi pu utiliser :\n\nFaire de même pour les itérations 1000 et 500000.\n\nprev1000 &lt;- predict(L2boost,newdata=df2,...)\nprev500000 &lt;- predict(L2boost,newdata=df2,...)\n...\n\n\nOn sur-ajuste lorsque le nombre d’itérations est trop important.\n\nSélectionner le nombre d’itérations par la procédure de votre choix.\n\nOn propose de faire une validation hold out. C’est assez facile avec gbm il suffit de renseigner l’option train.fraction de gbm.\n\n\nL2boost.sel &lt;- gbm(Y~.,data=df1,...)\ngbm.perf(...)\n\nReprésenter l’estimateur sélectionné.\n\nprev_opt &lt;- predict(L2boost.sel,newdata=df2,...)\n..."
  },
  {
    "objectID": "08-boosting.html#sec-gbmspam",
    "href": "08-boosting.html#sec-gbmspam",
    "title": "8  Gradient boosting",
    "section": "8.2 Adaboost et logitboost pour la classification binaire.",
    "text": "8.2 Adaboost et logitboost pour la classification binaire.\nOn considère le jeu de données spam du package kernlab.\n\nlibrary(kernlab)\ndata(spam)\nset.seed(1234)\nspam &lt;- spam[sample(nrow(spam)),]\n\n\nExécuter la commande et commenter la sortie.\n\nmodel_ada1 &lt;- gbm(type~.,data=spam,distribution=\"adaboost\",\n                  interaction.depth=2,\n                  shrinkage=0.05,n.trees=500)\n\n\nOn obtient le message d’erreur suivant :\n\nProposer une correction permettant de faire fonctionner l’algorithme.\n\nIl est nécessaire que la variable qualitative à expliquer soit codée 0-1 pour adaboost.\n\n\nspam1 &lt;- spam\nspam1$type &lt;- ...\n...    \n\nExpliciter le modèle ajusté par la commande précédente.\n\nL’algorithme gbm est une descente de gradient qui minimise la fonction de perte \\[\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,g(x_i)).\\] Dans le cas de adaboost on utilise la perte exponentielle : \\(\\ell(y,g(x))=\\exp(-yg(x))\\).\n\nEffectuer un summary du modèle ajusté. Expliquer la sortie.\n\nOn obtient un indicateur qui permet de mesurer l’importance des variable dans la construction de la méthode.\n\nUtiliser la fonction vip du package vip pour retrouver ce sorties.\nSélectionner le nombre d’itérations pour l’algorithme adaboost en faisant une validation croisée 5 blocs.\nPour l’estimateur sélectionné, calculer la prévision (label et probabilité d’être un spam) de l’individu suivant :\n\nxnew &lt;- spam[1000,-58]\n\n\nOn obtient la probabilité d’être spam avec\n\n\nOn prédira donc nonspam.\n\nFaire la même procédure en changeant la valeur du paramètre shrinkage (par exemple 0.05 et 0.5). Interpréter.\n\nLe nombre d’itérations optimal augmente lorsque shrinkage diminue. C’est logique car ce dernier paramètre contrôle la vitesse de descente de gradient : plus il est grand, plus on minimise vite et moins on itère. Il faut néanmoins veiller à ne pas le prendre trop petit pour avoir un estimateur stable. Ici, 0.05 semble être une bonne valeur.\n\nExpliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme.\n\nLa seule différence se situe au niveau de la fonction de perte, adaboost utilise \\[\\exp(-yg(x))\\] tandis que logitboost utilise \\[\\log(1+\\exp(-2yg(x)))\\] Avec gbm il faudra utiliser l’option distribution=\"bernoulli\" pour faire du logitboost, par exemple :"
  },
  {
    "objectID": "08-boosting.html#comparaison-de-méthodes",
    "href": "08-boosting.html#comparaison-de-méthodes",
    "title": "8  Gradient boosting",
    "section": "8.3 Comparaison de méthodes",
    "text": "8.3 Comparaison de méthodes\nOn reprend les données spam de l’exercice précédent et on les coupe en un échantillon d’apprentissage pour entraîner les algorithmes et un échantillon test pour les comparer :\n\nset.seed(1234)\nperm &lt;- sample(1:nrow(spam))\napp &lt;- spam[perm[1:3000],]\ntest &lt;- spam[-perm[1:3000],]\n\n\nSur les données d’apprentissage uniquement, entraîner\n\nl’algorithme adaboost en sélectionnant le nombre d’itérations par validation croisée\nl’algorithme logitboost en sélectionnant le nombre d’itérations par validation croisée\nune forêt aléatoire avec les paramètres par défaut\nun arbre CART\n\n\napp1 &lt;- app |&gt; mutate(type=as.numeric(type)-1)\nmodel_ada &lt;- gbm(type~.,data=app1,...)\nnb_ada &lt;- gbm.perf(...)\n\n\nmodel_logit &lt;- gbm(type~.,data=app1,...)\nnb_logit &lt;- gbm.perf(...)\n\n\nlibrary(ranger)\nforet &lt;- ranger(...,probability=TRUE)\nlibrary(rpart)\narbre &lt;- rpart(...)\ncp_opt &lt;- ...\narbre.opt &lt;- prune(...)\n\nPour les 4 algorithmes, calculer, pour tous les individus de l’échantillon test, la probabilité que ce soit un spam. On pourra stocker toutes ces probabilités dans un même tibble.\n\nprob &lt;- tibble(ada=predict(...,type=\"response\"),\n               logit=predict(...),\n               foret=predict(...)$prediction[,2],\n               arbre=predict(...)[,2],\n               obs=test$type)\n\nComparer les 3 algorithmes avec la courbe ROC, l’AUC et l’erreur de classification.\nComment aurait-on pu faire pour obtenir des résultats plus précis ?\n\nAvec une validation croisée plutôt qu’un simple découpage apprentissage/test."
  },
  {
    "objectID": "08-boosting.html#xgboost",
    "href": "08-boosting.html#xgboost",
    "title": "8  Gradient boosting",
    "section": "8.4 Xgboost",
    "text": "8.4 Xgboost\nL’algorithme xgboost Chen et Guestrin (2016) va plusloin que le gradient boosting en minimisant une approximation à l’odre 2 de la fonction de perte et en ajoutant un terme de régularisation dans la fonction objectif. On cherche toujours des combinaisons d’arbres\n\\[f_b(x)=f_{b-1}(x)+h_b(x)\\quad\\text{où}\\quad h_b(x)=w_{q(x)}\\]\nest un arbre à \\(T\\) feuilles : \\(w\\in\\mathbb R^T\\) et \\(q:\\mathbb R^d\\to\\{1,2,\\dots,T\\}\\). À l’étape \\(b\\), on cherche l’arbre qui minimise la fonction objectif de la forme\n\\[\n    \\begin{aligned}\n      \\text{obj}^{(b)}= & \\sum_{i=1}^n\\ell(y_i,f_b(x_i))+\\sum_{j=1}^b\\Omega(h_j) \\\\\n      = & \\sum_{i=1}^n\\ell(y_i,f_{b-1}(x_i)+h_b(x_i))+\\sum_{j=1}^b\\Omega(h_j)\n    \\end{aligned}\n\\]\noù \\(\\Omega(h_j)\\) est un terme de régularisation qui va pénaliser \\(h_j\\) en fonction de son nombre de feuilles \\(T\\) et des valeurs prédites \\(w\\). Un développement limité à l’ordre 2 permet d’approcher cette fonction par\n\\[\\text{obj}^{(b)}=\\sum_{i=1}^n[\\ell_i^{(1)}h_b(x_i)+\\frac{1}{2}\\ell_i^{(2)}h_b^2(x_i)]+\\Omega(h_b)+\\text{constantes},\\] avec\n\\[\\ell_i^{(1)}=\\frac{\\partial \\ell(y_i,f(x))}{\\partial f(x)}(f_{b-1}(x_i))\\quad\\text{et}\\quad \\ell_i^{(2)}=\\frac{\\partial^2 \\ell(y_i,f(x))}{\\partial f(x)^2}(f_{b-1}(x_i)).\\] Pour les arbres, la fonction de régularisation a la forme suivante :\n\\[\\Omega(h)=\\Omega(T,w)=\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^Tw_j^2,\\] où \\(\\gamma\\) et \\(\\lambda\\) contrôlent le poids que l’on donne aux paramètres de l’arbre. On obtient au final l’algorithme suivant\n\n\nInitialisation \\(f_0=h_0\\).\nPour \\(b=1,\\dots,B\\)\n\nAjuster un arbre \\(h_b\\) à \\(T\\) feuilles qui minimise \\[\\sum_{i=1}^n[\\ell_i^{(1)}h_b(x_i)+\\frac{1}{2}\\ell_i^{(2)}h_b^2(x_i)]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^Tw_j.\\]\nMettre à jour \\[f_b(x)=f_{b-1}(x)+h_b(x).\\]\n\nSortie : la suite d’algorithmes \\((f_b)_b\\).\n\n\nOn pourra trouver plus de précisions ici : https://xgboost.readthedocs.io/en/stable/tutorials/index.html\n\nExercice 8.1 (Prise en main des principales fonction de xgboost) On commence par charger le package\n\nlibrary(xgboost)\n\net on reprend les données sur le sinus de la Section 8.1 :\n\nx &lt;- seq(-2*pi,2*pi,by=0.01)\ny &lt;- sin(x)\nset.seed(1234)\nX &lt;- runif(200,-2*pi,2*pi)\nY &lt;- sin(X)+rnorm(200,sd=0.2)\ndf1 &lt;- data.frame(X,Y)\ndf2 &lt;- data.frame(X=x,Y=y)\n\nLa fonction xgboost requiert que les données possèdent la classe xgb.DMatrix, on peut l’obtenir avec\n\nX_mat &lt;- xgb.DMatrix(as.matrix(df1[,1]),label=df1$Y)\n\n\nExpliquer la sortie\n\nboost1 &lt;- xgboost(data=X_mat,nrounds = 5,\n                  params=list(objective=\"reg:squarederror\"))\n## [1]  train-rmse:0.633250 \n## [2]  train-rmse:0.468674 \n## [3]  train-rmse:0.354599 \n## [4]  train-rmse:0.275842 \n## [5]  train-rmse:0.224803\nboost1\n## ##### xgb.Booster\n## raw: 16.5 Kb \n## call:\n##   xgb.train(params = params, data = dtrain, nrounds = nrounds, \n##     watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, \n##     early_stopping_rounds = early_stopping_rounds, maximize = maximize, \n##     save_period = save_period, save_name = save_name, xgb_model = xgb_model, \n##     callbacks = callbacks)\n## params (as set within xgb.train):\n##   objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.print.evaluation(period = print_every_n)\n##   cb.evaluation.log()\n## niter: 5\n## nfeatures : 1 \n## evaluation_log:\n##  iter train_rmse\n##     1  0.6332497\n##     2  0.4686737\n##     3  0.3545990\n##     4  0.2758424\n##     5  0.2248031\n\n\nOn a ici entraîné xgboost avec la fonction de perte quadratique et 5 itérations.\n\nFaire la même chose avec 100 itération et un learning rate de 0.1.\nOn peut obtenir les valeurs prédites entre \\(-2\\pi\\) et \\(2\\pi\\) pour 100 itérations avec\n\nXtest &lt;- as.matrix(df2$X)\nprev100 &lt;- predict(boost2,newdata=Xtest,iterationrange = c(1,101))\n\nTracer les estimateurs xgboost pour 1 itération, 20 itérations et 100 itérations. Commenter.\n\nComme pour le gradient boosting, l’algorithme sous-apprend si le nombre d’arbres est trop petit et sur-apprend lorsqu’il est trop grand.\n\nCommenter la sortie\n\nset.seed(123)\nsel.xgb &lt;- xgb.cv(data = X_mat,\n                  nrounds = 100, objective = \"reg:squarederror\", \n                  eval_metric = \"rmse\",\n                  nfold=5,eta=0.1,\n                  early_stopping_rounds=10,\n                  verbose=FALSE)\nsel.xgb$evaluation_log |&gt; head()\n##    iter train_rmse_mean train_rmse_std test_rmse_mean\n## 1:    1       0.7894653    0.012973111      0.7906004\n## 2:    2       0.7189852    0.011889426      0.7228767\n## 3:    3       0.6556685    0.010877136      0.6607388\n## 4:    4       0.5985454    0.010104324      0.6055111\n## 5:    5       0.5471973    0.009379501      0.5567722\n## 6:    6       0.5007942    0.008971210      0.5129458\n##    test_rmse_std\n## 1:    0.05513590\n## 2:    0.05364182\n## 3:    0.05284791\n## 4:    0.05240736\n## 5:    0.05117564\n## 6:    0.04971156\n(ite.opt.xgb &lt;- sel.xgb$best_iteration)\n## [1] 27\nsel.xgb$niter\n## [1] 37\n\n\nOn effectue une validation croisée 5 blocs pour choisir le nombre d’itérations. L’argument early_stopping_rounds=10 permet de stopper l’algorithme lorsque l’erreur de prévision commence à trop remonter.\n\nTracer les prévisions pour l’algorithme sélectionné.\n\n\n\nExercice 8.2 (Xgboost sur les données spam) On reprend les données spam de la Section 8.2. Entraîner un algorithme xgboost avec la fonction de perte binary:logistic et en sélectionnant la nombre d’itérations par validation croisée en optimisant l’AUC. Attention cette fonction de perte requiert que la variable à expliquer prenne pour valeurs 0 ou 1 en classe numeric.\n\nOn teste la fonction xgboost pour voir si les donées sont au bon format.\n\n\nOn peut maintenant faire la valisation croisée :\n\n\nOn récupère les prévisions issues de la validation croisée\n\n\npour tracer la courbe ROC et calculer l’AUC :\n\n\n\nExercice 8.3 (Sélection avec tidymodels) Refaire l’exercice précédent avec la syntaxe tidymodels. On choisira notamment :\n\nla profondeur des arbres dans le vecteur\n\nc(1,3,8,10)\n\nle nombre d’itérations entre 1 et 500 avec un early_stopping toujours égal à 10 et un learning rate à 0.05.\n\nOn pourra consulter la page https://www.tidymodels.org/find/parsnip/ pour trouver les noms de paramètre du worfklow et sur le tutoriel https://juliasilge.com/blog/shelter-animals/ pour la stratégie. Elle est ici de fixer le nombre d’itérations à 500 puisqu’on utilise le early stopping en séparant les données en 2. On initialisera donc le workflow avec\n\nlibrary(tidymodels)\ntune_spec &lt;- \n  boost_tree(tree_depth=tune(),trees=500,learn_rate=0.05,\n             stop_iter=10) |&gt; \n  set_mode(\"classification\") |&gt;\n  set_engine(\"xgboost\",validation=0.2) \n\nxgb_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(type ~ .)\n\n\nOn définit la grille de paramètres et le ré-échantillonnage :\n\n\nOn fait la validation croisée :\n\n\nOn visualise les résultats et on choisit la meilleure valeur :\n\n\nOn finit en entraînant l’algorithme sur toute les données pur la valeur choisie :\n\n\nOn peut retrouver le nombre d’itérations sélectionnés par early stopping avec\n\n\nOn visualise enfin l’importance des variables avec\n\n\n\n\n\n\nChen, T., et C. Guestrin. 2016. « XGBoost: A Scalable Tree Boosting System ». In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785‑94. KDD ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785.\n\n\nFriedman, J. H. 2001. « Greedy Function Approximation: A Gradient Boosting Machine ». Annals of Statistics 29: 1189‑1232.\n\n\nRidgeway, G. 2006. « Generalized boosted models: A guide to the gbm package »."
  },
  {
    "objectID": "09-deep.html",
    "href": "09-deep.html",
    "title": "9  Réseaux de neurones avec Keras",
    "section": "",
    "text": "Nous présentons ici une introduction au réseau de neurones à l’aide du package keras. On pourra trouver une documentation complète ainsi qu’un très bon tutoriel aux adresses suivantes https://keras.rstudio.com et https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/. On commence par charger la librairie\n\nlibrary(keras)\n#install_keras() 1 seule fois sur la machine\n\nOn va utiliser des réseaux de neurones pour le jeu de données spam où le problème est d’expliquer la variable binaire typepar les 57 autres variables du jeu de données :\n\nlibrary(kernlab)\ndata(spam)\nspamX &lt;- as.matrix(spam[,-58])\n#spamY &lt;- to_categorical(as.numeric(spam$type)-1, 2)\nspamY &lt;- as.numeric(spam$type)-1\n\nOn sépare les données en un échantillon d’apprentissage et un échantillon test\n\nset.seed(5678)\nperm &lt;- sample(4601,3000)\nappX &lt;- spamX[perm,]\nappY &lt;- spamY[perm]\nvalidX &lt;- spamX[-perm,]\nvalidY &lt;- spamY[-perm]\n\n\nA l’aide des données d’apprentissage, entraîner un perceptron simple avec une fonction d’activation sigmoïde. On utilisera 30 epochs et des batchs de taille 5.\n\nOn définit tout d’abord la structure du réseau, 1 seule couche ici de 1 neurone :\n\n\nOn donne ensuite la fonction de perte, l’algorithme d’optimisation ainsi que le critère pour mesurer la performance du réseau :\n\n\nOn donne enfin dans fit les paramètres qui permettent d’entrainer le modèle (taille des batchs, nombre d’epochs…)\n\n\nLa fonction plot permet de visualiser la perte et la performance en fonction du nombre d’epochs :\n\n\n#Définition du modèle\npercep.sig &lt;- keras_model_sequential() \npercep.sig %&gt;% layer_dense(units=...,input_shape = ...,activation=\"...\")\nsummary(percep.sig)\npercep.sig %&gt;% compile(\n  loss=\"binary_crossentropy\",\n  optimizer=\"adam\",\n  metrics=\"accuracy\"\n)\n#Entrainement\np.sig &lt;- percep.sig %&gt;% fit(\n  x=...,\n  y=...,\n  epochs=...,\n  batch_size=...,\n  validation_split=...,\n  verbose=0\n)\n\nFaire de même avec la fonction d’activation softmax. On utilisera pour cela 2 neurones avec une sortie \\(Y\\) possédant la forme suivante.\n\nspamY1 &lt;- to_categorical(as.numeric(spam$type)-1, 2)\nappY1 &lt;- spamY1[perm,]\nvalidY1 &lt;- spamY1[-perm,]\n\nComparer les performances des deux perceptrons sur les données de validation à l’aide de la fonction evaluate.\nConstruire un ou deux réseaux avec deux couches cachées. On pourra faire varier les nombre de neurones dans ces couches. Comparer les performances des réseaux construits.\n\nOn propose tout d’abord 2 couches cachées composées de 100 neurones :\n\n\nOn propose ici 50 neurones pour la première couche cachée et 30 neurones pour la seconde. On ajoute de plus un dropout dans la première couche cachée (permet généralement d’éviter le sur-apprentissage, mais pas forcément utile ici).\n\n\nOn évalue la performance sur les données test :"
  },
  {
    "objectID": "10-dondes.html#critères-de-performance-pour-données-déséquilibrées",
    "href": "10-dondes.html#critères-de-performance-pour-données-déséquilibrées",
    "title": "10  Données déséquilibrées",
    "section": "10.1 Critères de performance pour données déséquilibrées",
    "text": "10.1 Critères de performance pour données déséquilibrées\nLa notion de risque en machine learning est capitale puisque c’est à partir de l’estimation de ces risques que l’on calibre des algorithmes et que l’on choisit un algorithme de prévision. En présence de données déséquilibré, il convient de choisir un risque adapté. En effet, il est le plus souvent important de parvenir à bien identifier des individus de la classe minoritaire. Des critères tels que l’accuracy ou l’erreur de classification ne sont pas pertinents pour ce cadre. On va privilégier des critères comme\n\nle balanced accuracy \\[\\text{Bal Acc}=\\frac{1}{2}\\mathbf P(g(X)=1|Y=1)+\\frac{1}{2}\\mathbf P(g(X)=-1|Y=-1)=\\frac{\\text{TPR+TNR}}{2}.\\]\nle \\(F_1\\)-score \\[F_1=2\\,\\frac{\\text{Precision }\\times\\text{Recall}}{\\text{Precision }+\\text{Recall}},\\] avec \\[\\text{Precision}=\\mathbf P(Y=1|g(X)=1)\\quad\\text{et}\\quad\\text{Recall}=\\mathbf P(g(X)=1|Y=1).\\]\nle kappa de Cohen \\[\\kappa=\\frac{\\mathbf P(a)-\\mathbf P(e)}{1-\\mathbf P(e)}\\] où \\(\\mathbf P(a)\\) représente l’accuracy et \\(\\mathbf P(e)\\) l’accuracy sous une hypothèse d’indépendance.\nla courbe ROC et l’AUC…\n\nComme d’habitude, ces critères sont inconnus et doivent être estimés par des méthodes de ré-échantillonnage de type validation croisée.\n\nExercice 10.1 (Calculer des critères)  \n\nGénérer un vecteur d’observations Y de taille 500 selon une loi de Bernoulli de paramètre 0.05.\nGénérer un vecteur de prévisions P1 de taille 500 selon une loi de Bernoulli de paramètre 0.01.\nGénérer un vecteur de prévision P2 de taille 500 tel que \\[\\mathcal L(P2|Y=0)=\\mathcal B(0.10)\\quad\\text{et}\\quad \\mathcal L(P2|Y=1)=\\mathcal B(0.85).\\]\nDresser les tables de contingence de P1 et P2 à l’aide de table. Commenter.\n\nOn remarque que P1 a tendance à prédire très souvent 0 (la classe majoritaire) alors que P2 est capable d’identifier plus d’invididus de la petite classe. Du point de vue de l’accuracy on va privilégier P1, néanmoins dans de nombreux cas P2 est plus pertinent.\n\nPour P2, calculer, avec les fonctions usuelles de R, l’accuracy, le recall et la précision.\nEn déduire le F1-score.\nMême question pour le \\(\\kappa\\) de Cohen.\nRetrouver ces indicateurs à l’aide des fonctions de package yardstick (voir https://yardstick.tidymodels.org/articles/metric-types.html#metrics). On pourra notamment utiliser la fonction metric_set. Utiliser également la fonction confusionMatrix de caret. ANalyser les prévisions P1 et P2.\n\nL’accuracy privilégie clairement P1 alors que d’autres critères comme le balanced accuracy, le F1-score ou le kappa de Cohen vont sélectionner P2. Ces derniers critères sont mieux adaptés pour prendre en considération la capacité à bien identifier la classe minoritaire."
  },
  {
    "objectID": "10-dondes.html#ré-équilibrage",
    "href": "10-dondes.html#ré-équilibrage",
    "title": "10  Données déséquilibrées",
    "section": "10.2 Ré-équilibrage",
    "text": "10.2 Ré-équilibrage\nEn complément du choix d’un critère pertinent, il peut être intéressant de tenter de ré-équilibrer l’échantillon pour aider les algorithmes à mieux détecter les individus de la classe minoritaire. Les méthodes classiques consistent à créer de nouvelles observations de la classe minoritaire (oversampling) et/ou supprimer des individus de la classe minoritaire (undersampling).\n\nExercice 10.2 (Quelques algorithmes de ré-équilibrage) On considère le jeu de données df ci-dessous où on cherche à prédire Y par X1 et X2.\n\nn &lt;- 2000\nset.seed(1234)\nX1 &lt;- runif(n)\nset.seed(5678)\nX2 &lt;- runif(n)\nset.seed(9012)\nR1 &lt;- X1&lt;=0.25\nR2 &lt;- (X1&gt;0.25 & X2&gt;=0.75)\nR3 &lt;- (X1&gt;0.25 & X2&lt;0.75)\nY &lt;- rep(0,n)\nY[R1] &lt;- rbinom(sum(R1),1,0.75)\nY[R2] &lt;- rbinom(sum(R2),1,0.75)\nY[R3] &lt;- rbinom(sum(R3),1,0.25)\ndf1 &lt;- tibble(X1,X2,Y)\ndf1$Y &lt;- factor(df1$Y)\nindDY1 &lt;- which(df1$Y==1)\ndf1.1 &lt;- df1[-indDY1[1:650],]\ndf1.2 &lt;- df1.1[sample(nrow(df1.1),1000),]\ndf &lt;- df1.2[sample(nrow(df1.2),100),]\nrownames(df) &lt;- NULL\np1 &lt;- ggplot(df)+aes(x=X1,y=X2,color=Y)+geom_point()\np1\n\n\n\n\nOn a ici 4 fois plus d’observations dans le groupe 0.\n\nsummary(df$Y)\n##  0  1 \n## 80 20\n\n\nOn commence par faire du oversampling avec la fonction step_upsample du package themis. On pourra s’inspire de https://github.com/tidymodels/themis.\n\nEffectuer le ré-échantillonnage et expliquer.\n\nlibrary(themis)\nrecipe(Y ~ ., data = df) |&gt; \n  step_upsample(Y) |&gt; \n  prep() |&gt; \n  bake(new_data = NULL) |&gt; \n  count(Y)\n## # A tibble: 2 × 2\n##   Y         n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 0        80\n## 2 1        80\n\n\nOn duplique des observations du groupe 1 pour atteindre le nombre d’observations du groupe 0.\n\nCorriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1.\n\nIl suffit de laisser intact le groupe 0 et de multiplier par 3 le nombre d’observations du groupe 1. Cela s’effectue avec l’option over_ratio.\n\n\nOn s’intéresse maintenant à l’algorithme SMOTE\n\nUtiliser la fonction step_smote avec k=3 et les autres valeurs de paramètres par défaut\n\n60 observations ont été crées par l’algorithme SMOTE.\n\nVisualiser les observations smote.\nCorriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1.\n\nIci encore il “suffit” de jouer avec l’option over_ratio.\n\n\nRefaire la question précedente avec l’algorithme ROSE (voir https://journal.r-project.org/archive/2014/RJ-2014-008/RJ-2014-008.pdf).\n\nToutes les observations des nouvelles données sont différentes des précédentes. On remarque que ce n’est plus le cas avec le jeu de paramètres suivant où la méthode ROSE devient un mélange de random oversampling et under sampling.\n\nOn souhaite maintenant ré-équilibrer par random undersampling. Utiliser la fonction stepdownsample pour effectuer un tel ré-équilibrage. Ici encore on pourra faire varier les paramètres.\n\nOn supprime des observations du groupe 0 pour avoir le même nombre d’observations dans les deux groupes. Ici encore on peut contrôler le niveau de ré-équilibrage avec under_ratio :\n\nOn passe maintenant à l’algorithme Tomek.\n\nSans utiliser la fonction TomekClassif identifier les paires d’observations qui ont un lien de Tomek. On pourra utiliser la fonction nng du package cccd.\n\nOn rappelle que deux observations ont un lien de Tomek si elles sont plus proches voisins mutuels et de deux groupes différents. On commence donc par identifier les plus proches voisins mutuels :\n\n\nPuis on récupère les groupes de ces observations afin de ne conserver que les paires qui proviennent de groupes différents :\n\nRetrouver ces paires à l’aide de la fonction step_tomek.\nVisualiser les observations supprimées.\n\n\n\n\nExercice 10.3 (Comparer des méthodes de ré-équilibrage avec tidymodels) On considère les données utilisées dans l’exemple https://www.tidymodels.org/learn/models/sub-sampling/ :\n\nimbal_data &lt;- \n  read_csv(\"https://tidymodels.org/learn/models/sub-sampling/imbal_data.csv\") |&gt; \n  mutate(Class = factor(Class))\ndim(imbal_data)\n## [1] 1200   16\nimbal_data |&gt; count(Class)\n## # A tibble: 2 × 2\n##   Class      n\n##   &lt;fct&gt;  &lt;int&gt;\n## 1 Class1    60\n## 2 Class2  1140\n\nClass1 est la classe minoritaire et représente l’évènement d’intérêt.\n\nCréer une recette permettant de ré-équilibrer les données avec la méthode ROSE (on utilisera les paramètres par défaut).\nOn souhaite appliquer l’algorithme des forêts aléatoires sur les données ré-équilibrées. Paramétrer l’algorithme en tidymodels. On utilisera comme valeurs de paramètres : mtry = 3, min_n = 1,trees = 1000.\nCréer un workflow qui combine la recette et l’algorithme des forêts aléatoires.\nÉvaluer les performances de la méthode en utilisant une validation croisée 10 blocs. On utilisera comme critère l’accuracy, l’index J de Youden, le kappa de Cohen, et l’auc.\nComparer ces performances avec l’algorithme sans ré-équilibrage. Interpréter.\n\nLe rééquilibrage n’améliore pas pour :\n\nl’accuracy : les données étant déséquilibrées, prédire toujours la classe majoritaire est performant pour ce critère ;\nl’AUC : ce critère est basé sur les probabilités et donc moins sensibles aux prévisions de groupe avec un seuil de 0.5. Il est en revanche beaucoup plus pertinent pour les deux autres critères. On peut expliquer cela en regardant lesvalauers prédites. Ré-équilibrer commet plus d’erreur de prévision mais permet de mieux détecter le classe minoritatire, ce qui est souvent le plus pertinent.\n\n\nEn vous inspirant de la syntaxe présentée ici https://www.tmwr.org/workflow-sets, comparer les performances de l’algorithme des forêts aléatoires utilisant différent type de ré-équilbrage (recettes), par exemple : oversampling, undersampling, smote, rose et aucun ré-équilibrage. On aggrégera touts ces recettes à l’aide de la fonction workflow_set.\n\nOn commence par préparer les recettes sur le ré-équilibrage.\n\n\nOn définit ensuite les paramètres de la forêt aléatoire.\n\n\nOn aggrège les recettes avec worflow_set.\n\n\nIl nous reste à définir les blocs (validation croisée 5 blocs répétée 10 fois)\n\n\net à lancer les validations croisées\n\n\nOn obtient les résultats suivants :\n\n\nLes méthodes de ré-équilibrage ont ici permis d’améliorer les différents critères à l’exception de l’accuracy (normal dans la mesure où ce critère n’est pas adapté aux données déséquilibrées). La méthode rose se retrouve régulièrement parmi les meilleurs approches. On peut donc l’extraire\n\n\n\n\nExercice 10.4 (Comparaison de méthodes de ré-équilibrage avec le package UBL) On considère 3 jeux de données df1, df2 et df3.\n\nn &lt;- 2000\nset.seed(12345)\nX1 &lt;- runif(n)\nset.seed(5678)\nX2 &lt;- runif(n)\nset.seed(9012)\nR1 &lt;- X1&lt;=0.25\nR2 &lt;- (X1&gt;0.25 & X2&gt;=0.75)\nR3 &lt;- (X1&gt;0.25 & X2&lt;0.75)\nY &lt;- rep(0,n)\nY[R1] &lt;- rbinom(sum(R1),1,0.75)\nY[R2] &lt;- rbinom(sum(R2),1,0.75)\nY[R3] &lt;- rbinom(sum(R3),1,0.25)\ndf1 &lt;- data.frame(X1,X2,Y)\ndf1$Y &lt;- factor(df1$Y)\nindDY1 &lt;- which(df1$Y==1)\ndf2 &lt;- df1[-indDY1[1:400],]\ndf3 &lt;- df1[-indDY1[1:700],]\ndf1 &lt;- df1[sample(nrow(df1),1000),]\ndf2 &lt;- df2[sample(nrow(df2),1000),]\ndf3 &lt;- df3[sample(nrow(df3),1000),]\n\n\nComparer la distribution de Y pour ces trois jeux de données et visualiser les observations.\n\nLes trois échantillons sont de même taille. Le groupe 0 est toujours plus important que le groupe 1 mais le déséquilibre est plus prononcé pour l’échantillon 2 et surtout l’échantillon 3.\n\nOn sépare ces 3 échantillons en un échantillon d’apprentissage et un échantillon test.\n\nset.seed(123)\nlibrary(caret)\na1 &lt;- createDataPartition(1:nrow(df1),p=2/3)\na2 &lt;- createDataPartition(1:nrow(df2),p=2/3)\na3 &lt;- createDataPartition(1:nrow(df3),p=2/3)\ntrain1 &lt;- df1[a1$Resample1,]\ntrain2 &lt;- df2[a2$Resample1,]\ntrain3 &lt;- df3[a3$Resample1,]\ntest1 &lt;- df1[-a1$Resample1,]\ntest2 &lt;- df2[-a2$Resample1,]\ntest3 &lt;- df3[-a3$Resample1,]\n\nAjuster une forêt aléatoire sur les 3 échantillon d’apprentissage, calculer les labels prédits sur les échantillons tests et estimer les différents indicateurs vus en cours à l’aide de confusionMatrix.\n\nOn remarque que l’accuracy est meilleur pour le 3ème échantillon, contrairement à des indicateurs tels que le \\(\\kappa\\) de Cohen ou le balanced accuracy.\n\nOn considère uniquement l’échantillon df3. Refaire l’analyse précédente en utilisant des techniques de ré-échantillonnage. On pourra utiliser les fonctions du package UBL.\n\nLe ré-équilibrage doit porter uniquement sur l’échantillon d’apprentissage. On propose d’utiliser le radom oversampling, smote, le random undersampling et tomek.\n\n\nOn entraîne les forêts aléatoires sur ces nouveaux échantillons et on prédit les individus de l’échantillon test\n\n\nOn en déduit les critères\n\n\nL’accuracy est sans surprise meilleur lorsqu’on ne ré-équilibre pas. Cependant les méthodes de ré-équilibrage permettent ici d’améliorer (plus ou moins) les autres critères."
  },
  {
    "objectID": "10-dondes.html#exercices-supplémentaires",
    "href": "10-dondes.html#exercices-supplémentaires",
    "title": "10  Données déséquilibrées",
    "section": "10.3 Exercices supplémentaires",
    "text": "10.3 Exercices supplémentaires\n\nExercice 10.5 (Echantillonnage rétrospectif) Dans le cadre de l’échantillonnage rétrospectif pour le modèle logistique vu en cours, démontrer la propriété qui lie le modèle logistique initial au modèle ré-équilibré.\n\nOn a \\[\\text{logit}\\, p_\\beta(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}\\quad\\text{et}\\quad \\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1|s_i=1)}{\\mathbf P(y_i=0|s_i=1)}.\\] Or \\[\\mathbf P(y_i=1|s_i=1)=\\frac{\\mathbf P(y_i=1,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=1)\\mathbf P(y_i=1)}{\\mathbf P(s_i=1)}\\] et \\[\\mathbf P(y_i=0|s_i=1)=\\frac{\\mathbf P(y_i=0,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=0)\\mathbf P(y_i=0)}{\\mathbf P(s_i=1)}.\\] Donc \\[\\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}+\\log\\frac{\\mathbf P(s_i=1|y_i=1)}{\\mathbf P(s_i=1|y_i=0)}=\\text{logit}\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1}}{\\tau_{0}}\\right).\\]\n\n\n\nExercice 10.6 (Echantillonnage rétrospectif) Une étude cas/témoins est réalisée pour mesurer l’effet du tabac sur une pathologie. Pour ce faire, on choisit \\(n_1=250\\) patients atteints de la pathologie (cas) et \\(n_0=250\\) patients sains (témoins). Les résultats de l’étude sont présentés ci-dessous\n\n\n\n\nFumeur\nNon fumeur\n\n\n\n\nNon malade\n48\n202\n\n\nMalade\n208\n42\n\n\n\n\nA partir des données obtenues, estimer à l’aide d’un modèle logistique la probabilité d’être atteint pour un fumeur, puis pour un non fumeur.\n\nPour simplifier on va construire un jeu de données qui correspond au résultat :\n\n\nOn peut maintenant estimer le modèle logistique et obtenir les prévisions demandées :\n\nComment interpréter ces deux probabilités ? Est-ce qu’elles estiment la probabilité d’être atteint pour un individu quelconque dans la population ?\n\nNon ! On a échantillonné de manière à avoir autant de patients malades que non malades, ce qui n’est pas vrai dans la population totale. On est face à un biais d’échantillonnage que l’on peut corriger à l’aide de l’exercice précédent.\n\nDes études précédentes ont montré que cinq individus sur mille sont atteints par la pathologie dans la population entière. En utilisant la propriété de l’exercice précédent, en déduire les probabilités d’être atteint pour un fumeur et un non fumeur dans la population.\n\nOn rappelle que \\[\\tau_1=\\mathbf P(S=1|Y=1)=\\frac{\\mathbf P(Y=1|S=1)\\mathbf P(S=1)}{\\mathbf P(Y=1)}.\\] Comme \\(\\mathbf P(Y=1|S=1)=\\mathbf P(Y=0|S=1)=1/2\\), on déduit \\[\\frac{\\tau_1}{\\tau_0}=\\frac{\\mathbf P(Y=0)}{\\mathbf P(Y=1)}=\\frac{\\pi_0}{\\pi_1}=\\frac{0.005}{0.995}.\\] On obtient ainsi les probabilités demandées avec\n\n\nOn peut retrouver les paramètres du modèle corrigé en utilisant l’option offset"
  },
  {
    "objectID": "11-comp-algos.html",
    "href": "11-comp-algos.html",
    "title": "11  Comparaison d’algorithmes",
    "section": "",
    "text": "Les chapitres précédents ont présenté plusieurs algorithmes permettant de répondre à un problème posé, le plus souvent de classification supervisée. Se pose bien entendu la question de choisir un unique algorithme. Etant donné un échantillon \\(\\mathcal D_n=\\{(x_1,y_1),\\dots,(x_n,y_y)\\}\\) on rappelle qu’un algorithme de prévision est une fonction \\[g:\\mathcal X\\times(\\mathcal X\\times \\mathcal Y)^n\\to\\mathcal Y\\] qui, à une nouvelle observation \\(x\\in\\mathcal X\\) renverra la prévision \\(g(x,\\mathcal D_n)\\) calculée à partir de l’échantillon \\(\\mathcal D_n\\). Cette fonction \\(g\\) peut contenir tout un tas d’étapes comme :\n\nla gestion des données manquantes\nune procédure de choix de variables\nune méthode pour ré-équilibrer les données\ndes procédures pour calibrer des paramètres (qui peuvent éventuellement inclure des validations croisées)\n…\n\nLe machine learning se focalisant sur la capacité d’un algorithme à bien prédire, les stratégies classiques pour choisir un algorithme vont (une fois de plus) consister à évaluer le pouvoir prédictif de chaque algorithme. Il n’y a rien de bien nouveau puisque cela va reposer sur les techniques présentées aux chapitres Chapitre 2 :\n\nchoisir un ou plusieurs critères (erreur de classification, AUC, \\(F_1\\)-score…)\nchoisir une procédure de ré-échantillonnage pour estimer ce critère (validation hold-out, validation croisée, OOB…).\n\nNous proposons de développer une stratégie pour choisir un algorithme sur le jeu de données Internet Advertisements Data Set disponible sur cette page https://archive.ics.uci.edu/ml/datasets/internet+advertisements. Le problème est d’identifier la présence d’une image publicitaire sur des pages webs. Il comporte\n\nad.data &lt;- read.table(\"data/ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",na.strings = \"?\",strip.white = TRUE)\ndim(ad.data)\n## [1] 3279 1559\n\nCe jeu de données contient 1558 variables explicatives, ces variables contiennent différentes caractériques de la page web (voir le site où sont présentées les données pour plus d’information). La dernière variable est la variable à expliquer, elle vaut ad. si présence d’une publicité, nonad. sinon.\n\nnames(ad.data)[ncol(ad.data)] &lt;- \"Y\"\nad.data$Y &lt;- as.factor(ad.data$Y)\nsummary(ad.data$Y)\n##    ad. nonad. \n##    459   2820\n\nCe jeu de données contient des données manquantes.\n\nsum(is.na(ad.data))\n## [1] 2729\n\nOn peut les visualiser avec\n\nlibrary(visdat)\nvis_miss(ad.data[,1:30])\n\n\n\n\nOn remarque que :\n\n920 lignes\n4 colonnes (les 4 premières)\n\nont au moins une valeur manquante.\n\napply(is.na(ad.data),1,any) %&gt;% sum()\n## [1] 920\nvar.na &lt;- apply(is.na(ad.data),2,any)\nnames(ad.data)[var.na]\n## [1] \"V1\" \"V2\" \"V3\" \"V4\"\n\nOn choisit de retirer ces 4 variables de l’analyse (il faudrait peut-être réfléchir un peu plus…).\n\nad.data1 &lt;- ad.data[,var.na==FALSE]\ndim(ad.data1)\n## [1] 3279 1555\nsum(is.na(ad.data1))\n## [1] 0\n\nOn se retrouve donc en présence de 3279 individus et 1554 variables explicatives. On construit la matrice des X et le vecteur des Y qui sont nécessaires pour certaines fonctions comme glmnet :\n\nX.ad &lt;- model.matrix(Y~.,data=ad.data1)[,-1]\nY.ad &lt;- ad.data1$Y\n\net on transforme la variable cible en 0-1 pour utiliser gbm:\n\nad.data2 &lt;- ad.data1 %&gt;% mutate(Y=recode(Y,\"ad.\"=0,\"nonad.\"=1))\n\nOn souhaite comparer les algorithmes présentés précédemment. Ils nécessitent les packages suivants\n\nlibrary(e1071)\nlibrary(caret)\nlibrary(rpart)\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(gbm)\n\nOn commence tout d’abord par représenter un algorithme par une fonction R qui admettra en entrée un jeu de données et renverra une unique prévision pour de nouveaux individus. On illustre ces fonctions pour prédire ce nouvel individu.\n\nnewX &lt;- ad.data1[1000,]\nnewX.X &lt;- matrix(X.ad[1000,],nrow=1)\n\nOn stockera les prévisions dans l’objet suivant\n\nprev &lt;- tibble(algo=c(\"SVM\",\"arbre\",\"ridge\",\"lasso\",\"foret\",\"ada\",\"logit\"),prev=0)\n\n\nSVM à noyau gaussien où le choix des paramètres du noyau se fait par validation croisée 4 blocs :\n\nprev.svm &lt;- function(df,newX){\n  C &lt;- c(0.01,1,10)\n  sigma &lt;- c(0.1,1,3)\n  gr &lt;- expand.grid(C=C,sigma=sigma)\n  ctrl &lt;- trainControl(method=\"cv\",number=4)\n  cl &lt;- makePSOCKcluster(3)\n  registerDoParallel(cl)\n  res.svm &lt;- train(Y~.,data=df,method=\"svmRadial\",trControl=ctrl,\n                   tuneGrid=gr,prob.model=TRUE)\n  stopCluster(cl)\n  predict(res.svm,newX,type=\"prob\")[2]\n}\nprev[1,2] &lt;- prev.svm(ad.data1,newX)\n\nArbre de classification où l’élagage est fait selon la procédure CART présentée dans le Chapitre 6.\n\nprev.arbre &lt;- function(df,newX){\n  arbre &lt;- rpart(Y~.,data=df,cp=1e-8,minsplit=2)\n  cp_opt &lt;- arbre$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% \n    dplyr::select(CP) %&gt;% slice(1) %&gt;% as.numeric()\n  arbre.opt &lt;- prune(arbre,cp=cp_opt)\n  predict(arbre,newdata=newX,type=\"prob\")[,2]\n}\nprev[2,2] &lt;- prev.arbre(ad.data1,newX)\n\nLasso et Ridge où le paramètre de régularisation est choisi par validation croisée 10 blocs en minimisant la déviance binomiale :\n\nprev.ridge &lt;- function(df.X,df.Y,newX){\n  ridge &lt;- cv.glmnet(df.X,df.Y,family=\"binomial\",alpha=0)\n  as.vector(predict(ridge,newx = newX,type=\"response\"))\n}\nprev.lasso &lt;- function(df.X,df.Y,newX){\n  lasso &lt;- cv.glmnet(df.X,df.Y,family=\"binomial\",alpha=1)\n  as.vector(predict(lasso,newx = newX,type=\"response\"))\n}\nprev[3,2] &lt;- prev.ridge(X.ad,Y.ad,newX.X)\nprev[4,2] &lt;- prev.lasso(X.ad,Y.ad,newX.X)\n\nForêt aléatoire avec les paramètres par défaut :\n\nprev.foret &lt;- function(df,newX){\n  foret &lt;- ranger(Y~.,data=df,probability=TRUE)\n  predict(foret,data=newX,type=\"response\")$predictions[,2]\n}\nprev[5,2] &lt;- prev.foret(ad.data1,newX)\n\nAdaboost et logitboost avec le nombre d’itérations choisi par validation croisée 5 blocs :\n\nprev.ada &lt;- function(df,newX){\n  ada &lt;- gbm(Y~.,data=df,distribution=\"adaboost\",interaction.depth=2,\n             bag.fraction=1,cv.folds = 5,n.trees=500)\n  nb.it &lt;- gbm.perf(ada,plot.it=FALSE)\n  predict(ada,newdata=newX,n.trees=nb.it,type=\"response\")\n}\n\nprev.logit &lt;- function(df,newX){\n  logit &lt;- gbm(Y~.,data=df,distribution=\"bernoulli\",interaction.depth=2,\n               bag.fraction=1,cv.folds = 5,n.trees=500)\n  nb.it &lt;- gbm.perf(logit,plot.it=FALSE)\n  predict(logit,newdata=newX,n.trees=nb.it,type=\"response\")\n}    \n\nprev[6,2] &lt;- prev.ada(ad.data2,newX)\nprev[7,2] &lt;- prev.logit(ad.data2,newX)\n\n\nOn peut visualiser la prévision de chaque algorithme\n\nprev\n## # A tibble: 7 × 2\n##   algo   prev\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 SVM       0\n## 2 arbre     0\n## 3 ridge     0\n## 4 lasso     0\n## 5 foret     0\n## 6 ada       0\n## 7 logit     0\n\n\nExercice 11.1 (Choix d’un algorithme par validation croisée) Choisir un algorithme parmi les précédents en utilisant comme critère l’erreur de classification ainsi que la courbe ROC et l’AUC. On pourra faire une validation croisée 10 blocs (même si ça peut être un peu long…).\n\nOn commence par définir les blocs\n\n\nOn effectue la validation croisée pour obtenir un score pour chaque individu :\n\nOn ajoute à cette matrice score les valeurs observées que l’on recode en 0-1 :\n\nOn peut maintenant déduire tous les critères en confondant les valeurs prédites aux valeurs observées.\n\n\nCourbe ROC\n\n\nAUC\n\n\nErreur de classification\n\n\nOn remarque que la svm possède les plus mauvais résultats. Cela ne signifie pas forcément que la méthode est mauvaise, peut-être que les choix qui ont été faits (noyaux gaussien, et grilles de paramètres) ne sont pas pertinents. Les arbres se révèlent également peu efficaces pour la courbe ROC et l’AUC, il est rare que les arbres soient parmi les meilleurs algorithmes contrairement au gradient boosting et aux forêts aléatoires. En terme d’AUC, la régression ridge et les forêts aléatoires se distinguent avec de très bonnes performances. On choisira l’algorithme final parmi ces deux là.\n\n\n\nExercice 11.2 (Choix d’un algorithme de ré-équilibrage par validation croisée) On considère le même jeu de données que précédemment. Choisir un algorithme de ré-équilibrage par validation croisée. Il s’agira de combiner des méthodes de ré-équilibrage (random over/under sampling, smote, tomek…) avec des algorithmes de prévision de machine learning. On pourra se restreindre au modèle logistique avec calcul des estimateurs par maximum de vraisemblance, ridge, lasso…\n\nOn recode les modalités de \\(Y\\) en 0-1 :\nOn définit d’abord les 10 blocs pour la validation croisée.\nPuis on crée une liste SCORE de longueur 5 dont chaque élément contiendra un dataframe avec les probabilités que l’image soit une publicité estimées par chaque méthode pour chaque algorithme de ré-équilibrage.\nOn peut maintenant faire la validation croisée, pour chaque valeur de k entre 1 et 10 :\n\non calcule les échantillons d’apprentissage et de test\non ré-équilibre les données d’apprentissage uniquement\non entraîne les algorithmes logistique (MV), lasso et ridge sur tous les échantillons d’apprentissage. Le paramètre de régularisation des méthode lasso et ridge est sélectionné en faisant une validation croisée 10 blocs, c’est-à-dire que l’échantillon d’apprentissage est coupé en 10 blocs pour choisir ce paramètre\non calcule enfin les probabilités estimées que l’image soit une publicité pour chaque individu de l’échantillon test\n\nLe code entier se trouve ci-dessous :\nOn assemble tous les résultats dans un seul dataframe de 4 colonnes qui contiendra une colonne :\n\nmeth pour le nom de l’algorithme de ré-équilibrage ;\nobs pour les valeurs observées de \\(Y\\) pour chaque individu (0 ou 1) ;\nalgo pour l’algorithme de prévision (logistique, lasso, ridge) ;\nscore pour la probabilité que l’image soit une publicité estimée par l’algorithme.\n\nOn pourra déduire de cette matrice différents critères de performance. Nous commençons par l’AUC qui peut se calculer directement à partir des valeurs de score\nLa plupart des autres critères (accuracy, kappa de Cohen…) nécessitent d’avoir les groupes prédits par l’algorithme. Ces prévisions s’obtiennent en fixant un seuil \\(s\\) entre 0 et 1 et en prédisant groupe 1 si la probabilité est plus grande que \\(s\\), 0 sinon. On peut utiliser des méthodes de choix automatique de seuil mais, par souci de concision, nous présentons les résultats pour le seuil de 0.5. On crée donc une nouvelle colonne prev qui contiendra les groupes prédits :\nOn peut maintenant en déduire :\n\nl’accuracy\nle balanced accuracy\nle \\(F_1\\) score\nle \\(\\kappa\\) de Cohen\n\nIl n’est pas facile de synthétiser tous ces résultats. Plusieurs tendances semblent néanmoins se dégager :\n\nle modèle logistique est clairement moins performant que les algorithmes ridge et lasso. Cela s’explique par le nombre important de variables dans le modèle. Les méthodes régularisées sont plus pertinentes dans ce contexte.\nles performances des régressions ridge et lasso sont proches, avec une préférence pour le ridge.\nl’apport des méthodes de ré-échantillonnage est discutable sur cet exemple. On remarque en effet les algorithmes appliqués sur les données brûtes (non ré-équilibrées) sont performants. Le ré-équilibrage peut néanmoins améliorer certains critères comme par exemple le balanced accuracy pour lequel les méthodes de sur-échantillonnage (random oversampling et smote) permettent d’améliorer.\n\nNous terminons en présentant quelques résultats pour d’autres valeurs de seuil. En effet, nous avons ici calculer la plupart des critères en utilisant le seuil de 0.5 pour déduire les groupes prédits à partir des valeurs de score. Il est souvent intéressant de visualiser ces critères pour différentes valeurs de seuil, on utilise alors des grilles de score qui consistent à présenter les valeurs de critère en fonction des scores. Nous comparons ici les grilles de score de la méthode ridge appliquée sur les données brûtes et les données ré-échantillonnées par random oversampling et smote. Ces grilles peuvent aider l’utilisateur à choisir le seuil de prévision en fonction des objectifs métiers. On créé tout d’abord une fonction qui permettra d’obtenir les grilles de score :"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Références",
    "section": "",
    "text": "Boehmke, B., and B. Greenwell. 2019. Hands-on Machine Learning with\nr. CRC Press. https://bradleyboehmke.github.io/HOML/.\n\n\nBreiman, L. 2001. “Random Forests.” Machine\nLearning 45: 5–32.\n\n\nBreiman, L., J. Friedman, R. Olshen, and C. Stone. 1984.\nClassification and Regression Trees. Wadsworth & Brooks.\n\n\nChen, T., and C. Guestrin. 2016. “XGBoost: A Scalable\nTree Boosting System.” In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n785–94. KDD ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785.\n\n\nFriedman, J. H. 2001. “Greedy Function Approximation: A Gradient\nBoosting Machine.” Annals of Statistics 29: 1189–1232.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2009. The Elements of\nStatistical Learning: Data Mining, Inference, and Prediction.\nSecond. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nRidgeway, G. 2006. “Generalized Boosted Models: A Guide to the Gbm\nPackage.”"
  }
]